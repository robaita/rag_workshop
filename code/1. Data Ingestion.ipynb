{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMyeJJMbn1v7e6xi6Y/yZVi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Ingestion\n","**Data ingestion** is the process of collecting and loading raw data from different sources into a system for downstream processing. In a RAG pipeline, it means **reading and preparing the content from PDFs, DOCX, XLSX, and web pages** into text format."],"metadata":{"id":"9JXf9iAjogQu"}},{"cell_type":"code","source":["!sudo apt-get update\n","!sudo apt-get install -y poppler-utils"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BaV4r21uwVS","executionInfo":{"status":"ok","timestamp":1748450496611,"user_tz":-330,"elapsed":18097,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"397fcf4f-5e10-486f-8a86-6f579ff55912"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n","Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,702 kB]\n","Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n","Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,735 kB]\n","Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,402 kB]\n","Get:14 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [34.3 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,564 kB]\n","Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,951 kB]\n","Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n","Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,987 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,264 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,552 kB]\n","Fetched 31.9 MB in 7s (4,876 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  poppler-utils\n","0 upgraded, 1 newly installed, 0 to remove and 63 not upgraded.\n","Need to get 186 kB of archives.\n","After this operation, 697 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n","Fetched 186 kB in 1s (151 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package poppler-utils.\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n","Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n","Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n","Processing triggers for man-db (2.10.2-1) ...\n"]}]},{"cell_type":"code","source":["!pip install -U unstructured-pytesseract\n","!pip install python-docx\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x6euyZY1vccg","executionInfo":{"status":"ok","timestamp":1748451155148,"user_tz":-330,"elapsed":7361,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"797ab23f-3a90-41f8-8eab-9d0863b12540"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: unstructured-pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.15)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from unstructured-pytesseract) (24.2)\n","Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-pytesseract) (11.2.1)\n","Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n"]}]},{"cell_type":"code","source":["!pip install -U langchain langchain-community unstructured pdf2image pytesseract python-docx openpyxl pdfminer.six pi_heif unstructured_inference"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"anOqZzkssm0C","executionInfo":{"status":"ok","timestamp":1748450618861,"user_tz":-330,"elapsed":119033,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"7ce4c180-29fc-4934-c23d-ca6aa3de39d5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n","Collecting langchain-community\n","  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n","Collecting unstructured\n","  Downloading unstructured-0.17.2-py3-none-any.whl.metadata (24 kB)\n","Collecting pdf2image\n","  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n","Collecting pytesseract\n","  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n","Collecting python-docx\n","  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n","Collecting pdfminer.six\n","  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n","Collecting pi_heif\n","  Downloading pi_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n","Collecting unstructured_inference\n","  Downloading unstructured_inference-1.0.2-py3-none-any.whl.metadata (5.3 kB)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.60)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n","Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n","  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n","Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.2.0)\n","Collecting filetype (from unstructured)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting python-magic (from unstructured)\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.9.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.4)\n","Collecting emoji (from unstructured)\n","  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n","Collecting python-iso639 (from unstructured)\n","  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n","Collecting langdetect (from unstructured)\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting rapidfuzz (from unstructured)\n","  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting backoff (from unstructured)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.2)\n","Collecting unstructured-client (from unstructured)\n","  Downloading unstructured_client-0.35.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.17.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.9.5)\n","Collecting python-oxmsg (from unstructured)\n","  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.2.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.2)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n","Collecting python-multipart (from unstructured_inference)\n","  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (0.31.4)\n","Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (4.11.0.86)\n","Collecting onnx (from unstructured_inference)\n","  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n","Collecting onnxruntime>=1.18.0 (from unstructured_inference)\n","  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (3.10.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (2.6.0+cu124)\n","Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (1.0.15)\n","Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (4.52.2)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (1.7.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (1.15.3)\n","Collecting pypdfium2 (from unstructured_inference)\n","  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n","Collecting coloredlogs (from onnxruntime>=1.18.0->unstructured_inference)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.18.0->unstructured_inference) (25.2.10)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.18.0->unstructured_inference) (5.29.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.18.0->unstructured_inference) (1.13.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n","  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured_inference) (3.18.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured_inference) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured_inference) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured_inference) (0.5.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->unstructured_inference) (2025.3.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->unstructured_inference)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->unstructured_inference)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->unstructured_inference)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->unstructured_inference)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->unstructured_inference)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->unstructured_inference)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->unstructured_inference)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->unstructured_inference)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->unstructured_inference)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->unstructured_inference)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (3.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.18.0->unstructured_inference) (1.3.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured) (2.7)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (1.17.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (0.5.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (2.9.0.post0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (1.5.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured_inference) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured_inference) (2025.2)\n","Collecting olefile (from python-oxmsg->unstructured)\n","  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm->unstructured_inference) (0.21.0+cu124)\n","Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n","  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.6.0)\n","Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n","  Downloading pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.18.0->unstructured_inference)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->unstructured_inference) (3.0.2)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n","Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured-0.17.2-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n","Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n","Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pi_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured_inference-1.0.2-py3-none-any.whl (47 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n","Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n","Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured_client-0.35.0-py3-none-any.whl (192 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.1/192.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n","Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=ad881c1adb4c2eb445064570ce2649edba7f323404b6c273d562db1bc83af4b8\n","  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n","Successfully built langdetect\n","Installing collected packages: filetype, rapidfuzz, python-multipart, python-magic, python-iso639, python-dotenv, python-docx, pytesseract, pypdfium2, pypdf, pi_heif, pdf2image, onnx, olefile, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, langdetect, humanfriendly, httpx-sse, emoji, backoff, aiofiles, typing-inspect, python-oxmsg, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, unstructured-client, pydantic-settings, pdfminer.six, onnxruntime, nvidia-cusolver-cu12, dataclasses-json, unstructured, unstructured_inference, langchain-community\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed aiofiles-24.1.0 backoff-2.2.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 emoji-2.14.1 filetype-1.2.0 httpx-sse-0.4.0 humanfriendly-10.0 langchain-community-0.3.24 langdetect-1.0.9 marshmallow-3.26.1 mypy-extensions-1.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 olefile-0.47 onnx-1.18.0 onnxruntime-1.22.0 pdf2image-1.17.0 pdfminer.six-20250506 pi_heif-0.22.0 pydantic-settings-2.9.1 pypdf-5.5.0 pypdfium2-4.30.1 pytesseract-0.3.13 python-docx-1.1.2 python-dotenv-1.1.0 python-iso639-2025.2.18 python-magic-0.4.27 python-multipart-0.0.20 python-oxmsg-0.0.2 rapidfuzz-3.13.0 typing-inspect-0.9.0 unstructured-0.17.2 unstructured-client-0.35.0 unstructured_inference-1.0.2\n"]}]},{"cell_type":"code","source":["from langchain.document_loaders import (\n","    UnstructuredPDFLoader,\n","    UnstructuredWordDocumentLoader,\n","    UnstructuredExcelLoader,\n","    WebBaseLoader\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zonP99Vo1Z-","executionInfo":{"status":"ok","timestamp":1748450621475,"user_tz":-330,"elapsed":2609,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"cd9b90ca-a2c2-408c-adc0-43e26a529bcd"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GCxuGojCsIKr","executionInfo":{"status":"ok","timestamp":1748450650543,"user_tz":-330,"elapsed":29070,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"48457ed3-67ce-4686-e900-cd90a6e8d2f6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Reading PDF files"],"metadata":{"id":"OUvD3JfmpES1"}},{"cell_type":"code","source":["pdf_loader = UnstructuredPDFLoader(\"/content/drive/MyDrive/Zeta Workshop/dataset/Avinash-CV.pdf\", mode=\"elements\")\n","elements = pdf_loader.load()\n"],"metadata":{"id":"b8jzUq1fo5tR","executionInfo":{"status":"ok","timestamp":1748450721335,"user_tz":-330,"elapsed":70794,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["page_data = {}\n","for idx, el in enumerate(elements):  # First 5 elements\n","    # print(f\"\\n--- Page {idx+1} ---\")\n","    # print(\"Available Metadata Keys:\", el.metadata.keys())\n","    # print(\"Metadata:\", el.metadata)\n","    # print(\"Page Number:\", el.metadata['page_number'])\n","    # print(\"Text:\", el.page_content)\n","    if el.metadata['page_number'] not in page_data:\n","      page_data[el.metadata['page_number']] = [el.page_content]\n","    else:\n","      page_data[el.metadata['page_number']].append(el.page_content)\n",""],"metadata":{"id":"iSN_fuumvR_-","executionInfo":{"status":"ok","timestamp":1748450930719,"user_tz":-330,"elapsed":45,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["for page in page_data:\n","  print(f\"Page {page}:\")\n","  print(page_data[page])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jZg9nXg8zAbj","executionInfo":{"status":"ok","timestamp":1748450958561,"user_tz":-330,"elapsed":30,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"21aff92e-c54a-4fc1-d7e0-5cd5d3814b6c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Page 1:\n","['Dr. Avinash Kumar Singh', 'Hyderabad, India | +91-9005722861 | avinashkumarsingh1986@gmail.com | http://avinashkumarsingh.in', 'Profile', 'With over 14 years in Al, I have evolved through roles as an ML Researcher, Engineer, Product Manager, and now as Chief Al Scientist. | have led the development and deployment of deep learning-based computer vision and NLP models on platforms like AWS, GCP, Humanoid Robots, Edge Devices like Jetson Nano, Raspberry Pi, and NXP boards. My expertise extends to tackling challenges in concurrency, security, and latency. My academic journey, enriched by a Ph.D. and postdoctoral research, provides a profound understanding of neural networks across diverse data types, while my industrial experience ensures practical AI solutions are deployed effectively, serving real users. This unique blend of research and industry expertise enables me to lead in crafting and delivering impactful Al innovations, driving business transformation and societal advancement.', 'Experience AI CONSULTANT & CORPORATE TRAINER | ROBAITA, HYDERABAD, INDIA | SEP 24 - CONT...', '= Mentored and empowered 1,500+ students and working professionals from diverse backgrounds. Led immersive sessions on Fine-tuning Large Language Models (LLMs), designing robust Retrieval-Augmented Generation (RAG) systems, building custom AI chatbots, defending LLMs against prompt injection attacks, implementing Model Context Protocol (MCP) for efficient context management, and developing Agentic Al solutions.', '= Engineered and deployed a cutting-edge visual language model-based computer vision system to drastically reduce loose picking of Apple iPhones ina warehouse environment, achieving a significant reduction from 20% to 5%. This innovative solution leverages visual question answering to enhance product identification, ensuring operational efficiency and accuracy.', '= Designed and developed an advanced conversational AI system tailored to resolve complex challenges within the supply chain. This intelligent system facilitates seamless communication with each supply chain node and accurately predicts arrival times by analyzing routes and external factors such as weather disruptions, protests, and regulatory conditions, ensuring timely and compliant operations.', '= Conceptualized and implemented \"Talk to Your Document,\" a generative Al solution that enables interactive question-and-answer sessions with historical documents and databases. This solution achieved impressive BLEU and ROUGE scores of 0.85, demonstrating exceptional accuracy in retrieving and synthesizing relevant information.', 'GLOBAL SOLUTION LEADER | BRANE ENTERPRISES LLP, HYDERABAD, INDIA | MAY 20 - SEP 24', '= A LLM model is finetuned on financial reports to understand complex financial reasoning (mostly tables). The system could generate the mathematical formula to process the query, retrieve the argument from the table and compute the value as the output.', '= Implemented a Face Recognition-based office attendance system, replacing the existing RFID system, and achieving organization-wide deployment.', '= The system serves 2,856 employees with a 97.63% accuracy rate, resulting in annual savings in operational costs.', '= Leda groundbreaking project to design smart glasses for visually impaired individuals, providing comprehensive assistance in reading, navigation, currency identification, person recognition, and scene understanding.', '= The system can detect obstacle up to 5 feet, can help in reading English and six Indian languages, could recognize 9,605 objects and labels with 89.76% accuracy.', '= Successfully delivered a Driver Monitoring System (DMS), incorporating real-time monitoring and safety features. The system tracks driver drowsiness, smoking, drinking, eating, phone usage, and seatbelt compliance, resulting in a 40% reduction in driving violations.', '= Captured and recorded over 1,000 violations with images, date, time, and other details.', '= Provided live streams from both interior and exterior dash cameras, enhancing monitoring accuracy.']\n","Page 2:\n","['SENIOR RESEARCHER | MONTPELLIER UNIVERSITY, FRANCE | NOV 20- NOV 21', '= | was associated with the robotics lab (LIRMM) and worked on the EU Project SOPHIA. As the in-charge of Work package 5, I helped my team to coordinate between different project partners Italian Institute of Technology, Italy, INAIL, Italy, and LIRMM, France for data acquisition, human-robot interaction and to derive a deep learning model', 'for action recognition.', '= Designed and developed a sensor agnostic, Bidirectional LSTM based deep neural network for action recognition. The model is tested in the presence of Xsens suit (used for motion capture) and Intel RealSense and Microsoft Kinect RGB-D data (3D skeleton). The research is published in 21st International Conference on Humanoid Robots.', '= The model is integrated with KUKA robot to help human in physical assistance, e.g. carrying object, release object, place object etc. in industrial environment by understanding the human actions.', 'POST DOCTORAL RESEARCHER | UMEA UNIVERSITY, SWEDEN | FEB 18 - JAN 20', '= During this postdoc, | closely worked with Professor Kai-Florian Richter and Professor Thomas Hellstrém. | was a part of intelligent robotics lab, during this postdoc, we designed a dialogue based human robot interaction system that allows humans, to talk to the robot. This work was published in a ‘A’ rated conference ECAI-2020.', '= We developed and implemented a robot collaboration framework that enables robots to have dialogues by translating their actions into the natural language. This work was published in Journal of behavioural robotics and also featured in Softbank robotics under the best 20 projects in 2020.', 'DEPUTY MANAGER | HCL TECHNOLOGIES, NOIDA, INDIA | FEB 17 - JAN 18', '= | joined the HCL machine learning division (Noida) when this was a 3 members team. In the span of one year, we conducted 4 successful POCs and grew the team to a 16 members team.', '= Asa deputy manager my responsibilities were to handle the client interaction, project scoping, find the place where machine learning solutions can be pitched (to integrated with existing workflows) within and outside the organization.', '= Helped my team to set up the machine learning GPU infrastructure, sketching project roadmaps, resource allocation and tracking. Further motivating team to follow the software engineering best practices such as maintaining git, hygiene of code (following the coding standards), etc.', 'ML ENGINEER | ECLERX SERVICES LIMITED, MUMBAI, INDIA | NOV 15 - FEB 17', '= eClerx is an Indian IT consulting and outsourcing multinational company. I worked there as a full stack developer and my job was to design and develop NLP, ML solutions. We used image pre-processing to improve OCR accuracy, further to integrate these solutions with RPA system.', 'Education PH.D | COMPUTER VISION | DEC 2016] IIIT, ALLAHABAD, INDIA', 'M.TECH | INFORMATION SECURITY | JUNE 2011 | KIIT UNIVERSITY, BHUBANESHWAR, INDIA M.SC | INFORMATION TECHNOLOGY | JUNE 2009 | KUMAUN UNIVERSITY, ALMORA, INDIA', 'B.SC | MATHEMATICS | JUNE 2007 | KUMAUN UNIVERSITY, ALMORA, INDIA', 'Skills & Abilities', '= Machine Learning = Machine learning operations (MLOps) = Computer vision = Certified scrum master', '= Natural language processing = AWS, GCP, E2E cloud platforms', '= Generative Al, LLM, Vision Language Model = Python, TensorFlow, Pytorch, MySQL', 'Page 2']\n","Page 3:\n","['= Genetic Algorithms, Fuzzy Systems = Human-Robot Interactions (Nao, Pepper)', 'Certifications', '= Machine Learning Operations (MLOps) Coursera - Deeplearnining.ai Focus Areas - ML Workflows, TFX, Model Deployments and Tracking for Production May 3, 2022 = TensorFlow 2 for Deep Learning Coursera - Imperial College London Focus Areas — TensorFlow 2 APIs, Customized Model Training, Probabilistic Deep Learning Jul 9, 2023 = Machine Learning and Soft Computing Indian Statistical Institute, Kolkata, India Focus Areas - Artificial Neural Network, Computer Vision and Genetic Algorithms Dec 20, 2012', 'Achievements', '= SOPHIA - A H2020 EU PROJECT .', 'Secured three years post-doctoral position on deep learning perception for human-robot collaboration at LIRMM Montpellier.', '= POST-DOCTORAL FUNDING FROM KEMPHE .', 'FOUNDATIONS, SWEDEN Received 2 years of research funding to pursue my postdoctoral work in Human-Robot Interaction at Umea University, Sweden.', '= 1ST POSITION IN M.SC Secured first position in M.Sc for consecutive 2 years at university level.', 'Projects', 'Git, CI/CD, Dockers, Micro Services (REST), JIRA 10T - ESP32, MQTT, Jetson Nano, Raspberry, NXP', 'Building Cloud Computing Solutions at Scale Coursera - Duke University', 'Focus Areas - Containers, Cloud APIs, DevOps, AWS an Azure', 'Nov 6, 2023', 'Scrum Master Certification Specialization Coursera — Learn Quest', 'Focus Areas - Agile Methodology and Product Management', 'Feb 17, 2023', 'Prompt Engineering for ChatGPT', 'Vanderbilt University', 'Focus Area - Few Shot Examples, Meta Language Creation Pattern, Different prompts patterns July 9, 2023', 'MARIE SKLODOWSKA CURIE ACTIONS (MSCA) 2020 Applied for the MSCA post-doctoral funding in association with Universitat Rovira i Virgili, Spain and received 85% marks.', 'MHRD, INDIA PH.D FELLOWSHIP', 'Received 2 years of junior research and 2 years of senior research funding from MHRD for the Ph.D program under robotics and AI lab, IIIT Allahabad.', 'FINETUNENING OF LLM FOR Q&A ON WEBSITE DATA', \"In today's world, simply having an FAQ section or static information on a website isn't enough. We can harness this data to build a conversational Al system that provides more effective and intuitive search and information retrieval. To tackle this, we fine-tuned the LLaMA model using data scraped from the website. Initially, we generated question- answer pairs using the website content and ChatGPT. Once we had sufficient data, we used it to fine-tune the model. The model was then deployed as a conversational Al system using OpenWebUI, enabling users to engage with the content in a more interactive manner.\", 'Techniques Used: Llama, OpenWebUI, LoRA, Data Preparation, Tokenization and Encoding.', 'Page 3']\n","Page 4:\n","['FINANCE LARGE LANGUAGE MODEL', 'We fine-tuned BERT, an LLM model, to handle complex reasoning tasks found in financial reports. We utilized the FinQA dataset, which consists of earnings reports from S&P 500 companies spanning from 1999 to 2019. These reports, typically in PDF format, include multiple pages of financial data, often presented in tables and text. The model generates the mathematical equation necessary to conduct the calculation and further execute that equation on table selected values to get the output value (execution value). The model achieved an execution accuracy of 65.05%.', 'Techniques Used: BERT, Language Embedding, Program Accuracy, Execution Accuracy', 'SMART VISION - BRANE ENTERPRISES', 'We created a smart eyewear prototype equipped with an integrated camera to aid visually impaired individuals. Our in-house fabrication of the PCB board enables essential functionalities like Bluetooth connectivity, USB charging, and live-streaming to an Android device. Additionally, we developed a companion app providing features such as object detection, scene analysis, navigation assistance, and person counting. My role involved collaborating with the team to develop and integrate deep learning solutions into the app. Together, we successfully implemented multiple models for object detection, image/dense captioning, currency recognition, OCR, and more.', 'Techniques Used: Faster-RCNN, MobileNetV2, CNN+LSTM, YOLO', 'INCREMENTAL FACE LEARNING - BRANE ENTERPRISES', 'We developed an Android application for conducting face recognition using transfer learning techniques. We utilized the FaceNet model as the backbone network and added a classification layer for face classification. One challenge with neural networks is catastrophic forgetting, where training the entire network (excluding the backbone) on N+1 classes can lead to performance issues. To address this, we adopted an incremental learning approach that helps to train and deploy the model in 30 seconds reducing the onboarding time.', 'Techniques Used: Transfer Learning, FaceNet, Incremental Learning', 'ACTION RECOGNITION - MONTPELLIER UNIVERSITY', 'Given one second long measure of the human’s motion, the system can determine human action. The originality lies in the use of joint angles, instead of cartesian coordinates. This design choice makes the framework sensor agnostic and invariant to affine transformations and to anthropometric differences. On AnDy dataset, we outperform the state of the art classifier. Furthermore, we show that our system is effective with limited training data, that it is subject independent, and that it is compatible with robotic real time constraints. In terms of methodology, the system is an original synergy of two antithetical schools of thought: model based and data-based algorithms. Indeed, it is the cascade of an inverse kinematics estimator compliant with the International Society of Biomechanics recommendations, followed by a deep learning architecture based on Bidirectional Long Short Term Memory. Techniques Used: Bi-LSTM, Mocap, OpenPose, CNN', 'VISUAL GROUNDING - UMEA UNIVERSITY', 'For robots to engage with humans in real-world situations or with objects, they must develop a mental representation (\"state of mind\") that a) reflects the robots’ perception and b) ideally aligns with human comprehension and ideas. Using table-top scenarios as an example, we propose a framework for generating a robot\\'s \"state of mind” by identifying the objects on the table along with their characteristics (color, shape, texture) and spatial relationships to one another. The robot\\'s view of the scene is depicted in a dynamic graph where object attributes are translated into fuzzy linguistic variables that correspond to human spatial concepts. This endeavor involves creating these graph representations through a combination of low-level neural network-based feature recognition and a high-level fuzzy inference system.', 'Techniques Used: Fuzzy Inference System, Mask-RCNN, CNN, Local Binary pattern (LBP), Multi-Layer Perceptron', 'Page 4']\n","Page 5:\n","['Talks and Presentations', '= Title: An empirical review of calibration techniques for the Pepper humanoid robot’s RGB and depth camera. Venue: Intelligent systems and application, 5‘ Sep 2019, London, England. Occasion: Presented conference paper in IntelliSys 2019. = Fusion of gesture and speech for increased accuracy in human robot interaction. Venue: 25th International conference on methods and models in automation and robotics, 24 Aug 2019, Miedzyzdroje, Poland. Occasion: Presented conference paper in MMAR 2019. = Conflict Detection and Resolution in Table Top Scenarios for Human-Robot Interaction. Venue: Computing science department, Umea University, 18 Jun, 2019, Umea, Sweden. Occasion: Poster presentation in 31st Swedish Al Society Workshop. = Deep learning and its applications. Venue: UFBI department, Umea University, 15\" Jun 2018, Umea, Sweden. Occasion: Invited as a speaker at Umea center for Functional Brain Imaging (UFBI) day. = Sketch drawing by NAO humanoid robot. Venue: TENCON a premier international technical conference of IEEE Region 10, 1st Nov, 2015, Macau, China. Occasion: Presented conference paper in TENCON 2015.', 'Selected Publications [ Journals and Conferences]', 'JOURNAL PUBLICATIONS', '= Singh, A. K., Baranwal, N., Richter, K. F., Hellstrom, T., & Bensch, S. (2020). Verbal explanations by collaborating robot teams. Paladyn, Journal of Behavioral Robotics, 12(1), 47-57.', '= Singh, A. K., Baranwal, N., & Nandi, G. C. (2019). A rough set based reasoning approach for criminal identification. International Journal of Machine Learning and Cybernetics, 10, 413-431.', '= Baranwal, N., Nandi, G.C., & Singh, A. K. (2017). Real-Time Gesture-Based Communication Using Possibility Theory- Based Hidden Markov Model. Computational Intelligence, 33(4), 843-862.', '= Baranwal, N., Singh, A. K., & Nandi, G. C. (2017). Development of a framework for human-robot interactions with Indian sign language using possibility theory. International Journal of Social Robotics, 9, 563-574.', '= Singh, A. K., Baranwal, N., & Nandi, G. C. (2017). Development of a self reliant humanoid robot for sketch drawing. Multimedia Tools and Applications, 76, 18847-18870.', 'CONFERENCE PUBLICATIONS', '= Singh, A. K.,, Adjel, M., Bonnet, V., Passama, R., & Cherubini, A. (2022, November). A framework for recognizing industrial actions via joint angles. In 2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids) (pp. 210-216). IEEE.', '= Kumar Singh, A., Baranwal, N., & Richter, K. F. (2020). A fuzzy inference system for a visually grounded robot state of mind. In ECAI 2020 (pp. 2402-2409). IOS Press.', '= Singh, A. K., Baranwal, N., & Richter, K. F. (2020). An empirical review of calibration techniques for the pepper humanoid robot’s RGB and depth camera. In Intelligent Systems and Applications: Proceedings of the 2019 Intelligent Systems Conference (IntelliSys) Volume 2 (pp. 1026-1038). Springer International Publishing.', '= Singh, A. K., Chakraborty, P., & Nandi, G. C. (2015, November). Sketch drawing by nao humanoid robot. In TENCON 2015-2015 IEEE Region 10 Conference (pp. 1-6). IEEE.', 'Please find the complete list of publication at my Google Scholar Profile.', 'Online Presence', 'Github: https://github.com/robaita LinkedIn: https://fr.linkedin.com/in/dr-avinash-kumar-singh-2a570a31', 'Page 5']\n"]}]},{"cell_type":"markdown","source":["# Reading Word files"],"metadata":{"id":"sAdsDxV5rqhS"}},{"cell_type":"code","source":["# docx_loader = UnstructuredWordDocumentLoader(\"/content/drive/MyDrive/Zeta Workshop/dataset/Avinash-CV.docx\")\n","# docx_docs = docx_loader.load()\n","\n","from docx import Document\n","\n","def extract_docx_structure(docx_path):\n","    doc = Document(docx_path)\n","    structure = []\n","\n","    for i, para in enumerate(doc.paragraphs):\n","        text = para.text.strip()\n","        style = para.style.name\n","\n","        if not text:\n","            continue\n","\n","        element = {\n","            \"paragraph_num\": i + 1,\n","            \"text\": text,\n","            \"style\": style\n","        }\n","\n","        # Identify heading level\n","        if style.startswith(\"Heading\"):\n","            element[\"heading_level\"] = style\n","\n","        structure.append(element)\n","\n","    return structure\n","\n","doc_structure = extract_docx_structure(\"/content/drive/MyDrive/Zeta Workshop/dataset/Avinash-CV.docx\")\n","\n","for item in doc_structure:\n","    print(f\"\\nParagraph {item['paragraph_num']}\")\n","    print(f\"Style: {item['style']}\")\n","    if \"heading_level\" in item:\n","        print(f\"Heading: {item['heading_level']}\")\n","    print(f\"Text: {item['text'][:200]}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7B-ZKROIruc8","executionInfo":{"status":"ok","timestamp":1748451219163,"user_tz":-330,"elapsed":102,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"cd196333-114c-404d-ed2d-94c99a7b1b85"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Paragraph 1\n","Style: Contact\n","Text: Hyderabad, India | +91-9005722861 | avinashkumarsingh1986@gmail.com\t | http://avinashkumarsingh.in\n","\n","Paragraph 3\n","Style: Heading 1\n","Heading: Heading 1\n","Text: With over 14 years in AI, I have evolved through roles as an ML Researcher, Engineer, Product Manager, and now as Chief AI Scientist. I have led the development and deployment of deep learning-based c\n","\n","Paragraph 6\n","Style: Heading 2\n","Heading: Heading 2\n","Text: AI Consultant & corporate trainer | robaita, Hyderabad, India | SeP 24 – CONT…\n","\n","Paragraph 7\n","Style: List Bullet\n","Text: Mentored and empowered 1,500+ students and working professionals from diverse backgrounds. Led immersive sessions on Fine-tuning Large Language Models (LLMs), designing robust Retrieval-Augmented Gene\n","\n","Paragraph 8\n","Style: List Bullet\n","Text: Engineered and deployed a cutting-edge visual language model-based computer vision system to drastically reduce loose picking of Apple iPhones in a warehouse environment, achieving a significant reduc\n","\n","Paragraph 9\n","Style: List Bullet\n","Text: Designed and developed an advanced conversational AI system tailored to resolve complex challenges within the supply chain. This intelligent system facilitates seamless communication with each supply \n","\n","Paragraph 10\n","Style: List Bullet\n","Text: Conceptualized and implemented \"Talk to Your Document,\" a generative AI solution that enables interactive question-and-answer sessions with historical documents and databases. This solution achieved i\n","\n","Paragraph 11\n","Style: Heading 2\n","Heading: Heading 2\n","Text: Global solution leader | Brane ENterprises LLP, hyderabad, india | May 20 – SeP 24\n","\n","Paragraph 12\n","Style: List Bullet\n","Text: A LLM model is finetuned on financial reports to understand complex financial reasoning (mostly tables). The system could generate the mathematical formula to process the query, retrieve the argument \n","\n","Paragraph 13\n","Style: List Bullet\n","Text: Implemented a Face Recognition-based office attendance system, replacing the existing RFID system, and achieving organization-wide deployment.\n","\n","Paragraph 14\n","Style: List Bullet\n","Text: The system serves 2,856 employees with a 97.63% accuracy rate, resulting in annual savings in operational costs.\n","\n","Paragraph 15\n","Style: List Bullet\n","Text: Led a groundbreaking project to design smart glasses for visually impaired individuals, providing comprehensive assistance in reading, navigation, currency identification, person recognition, and scen\n","\n","Paragraph 16\n","Style: List Bullet\n","Text: The system can detect obstacle up to 5 feet, can help in reading English and six Indian languages, could recognize 9,605 objects and labels with 89.76% accuracy.\n","\n","Paragraph 17\n","Style: List Bullet\n","Text: Successfully delivered a Driver Monitoring System (DMS), incorporating real-time monitoring and safety features. The system tracks driver drowsiness, smoking, drinking, eating, phone usage, and seatbe\n","\n","Paragraph 18\n","Style: List Bullet\n","Text: Captured and recorded over 1,000 violations with images, date, time, and other details.\n","\n","Paragraph 19\n","Style: List Bullet\n","Text: Provided live streams from both interior and exterior dash cameras, enhancing monitoring accuracy.\n","\n","Paragraph 20\n","Style: Heading 2\n","Heading: Heading 2\n","Text: senior Researcher | montpellier university, france | Nov 20- Nov 21\n","\n","Paragraph 21\n","Style: List Bullet\n","Text: I was associated with the robotics lab (LIRMM) and worked on the EU Project SOPHIA. As the in-charge of Work package 5, I helped my team to coordinate between different project partners Italian Instit\n","\n","Paragraph 22\n","Style: List Bullet\n","Text: Designed and developed a sensor agnostic, Bidirectional LSTM based deep neural network for action recognition. The model is tested in the presence of Xsens suit (used for motion capture) and Intel Rea\n","\n","Paragraph 23\n","Style: List Bullet\n","Text: The model is integrated with KUKA robot to help human in physical assistance, e.g. carrying object, release object, place object etc. in industrial environment by understanding the human actions.\n","\n","Paragraph 24\n","Style: Heading 2\n","Heading: Heading 2\n","Text: post doctoral Researcher | Umeå university, sweden | feb 18 - jan 20\n","\n","Paragraph 25\n","Style: List Bullet\n","Text: During this postdoc, I closely worked with Professor Kai-Florian Richter and Professor Thomas Hellström. I was a part of intelligent robotics lab, during this postdoc, we designed a dialogue based hum\n","\n","Paragraph 26\n","Style: List Bullet\n","Text: We developed and implemented a robot collaboration framework that enables robots to have dialogues by translating their actions into the natural language. This work was published in Journal of behavio\n","\n","Paragraph 27\n","Style: Heading 2\n","Heading: Heading 2\n","Text: deputy manager | hcl technologies, noida, india | feb 17 - jan 18\n","\n","Paragraph 28\n","Style: List Bullet\n","Text: I joined the HCL machine learning division (Noida) when this was a 3 members team. In the span of one year, we conducted 4 successful POCs and grew the team to a 16 members team.\n","\n","Paragraph 29\n","Style: List Bullet\n","Text: As a deputy manager my responsibilities were to handle the client interaction, project scoping, find the place where machine learning solutions can be pitched (to integrated with existing workflows) w\n","\n","Paragraph 30\n","Style: List Bullet\n","Text: Helped my team to set up the machine learning GPU infrastructure, sketching project roadmaps, resource allocation and tracking. Further motivating team to follow the software engineering best practice\n","\n","Paragraph 31\n","Style: Heading 2\n","Heading: Heading 2\n","Text: ML Engineer | eClerx services limited, mumbai, india | nov 15 - feb 17\n","\n","Paragraph 32\n","Style: List Bullet\n","Text: eClerx is an Indian IT consulting and outsourcing multinational company. I worked there as a full stack developer and my job was to design and develop NLP, ML solutions. We used image pre-processing t\n","\n","Paragraph 34\n","Style: Heading 2\n","Heading: Heading 2\n","Text: PH.D | Computer Vision | Dec 2016| IIIT, allahabad, india\n","\n","Paragraph 36\n","Style: Heading 2\n","Heading: Heading 2\n","Text: m.tech | information SECURITY |  | kiit university, bhubaneshwar, india\n","\n","Paragraph 38\n","Style: Heading 2\n","Heading: Heading 2\n","Text: m.Sc | information technology | june 2009 | kumaun university, almora, india\n","\n","Paragraph 40\n","Style: Heading 2\n","Heading: Heading 2\n","Text: b.sc | mathematics | june 2007 | kumaun university, almora, india\n","\n","Paragraph 42\n","Style: Heading 1\n","Heading: Heading 1\n","Text: Certifications\n","\n","Paragraph 43\n","Style: Heading 1\n","Heading: Heading 1\n","Text: Achievements\n","\n","Paragraph 44\n","Style: Heading 1\n","Heading: Heading 1\n","Text: Projects\n","\n","Paragraph 45\n","Style: Heading 2\n","Heading: Heading 2\n","Text: Finetunening of LLM for Q&A on Website Data\n","\n","Paragraph 46\n","Style: List Bullet\n","Text: In today's world, simply having an FAQ section or static information on a website isn't enough. We can harness this data to build a conversational AI system that provides more effective and intuitive \n","\n","Paragraph 47\n","Style: List Bullet\n","Text: Techniques Used: Llama, OpenWebUI, LoRA, Data Preparation, Tokenization and Encoding.\n","\n","Paragraph 48\n","Style: Heading 2\n","Heading: Heading 2\n","Text: FinanCE Large Language Model\n","\n","Paragraph 49\n","Style: List Bullet\n","Text: We fine-tuned BERT, an LLM model, to handle complex reasoning tasks found in financial reports. We utilized the FinQA dataset, which consists of earnings reports from S&P 500 companies spanning from 1\n","\n","Paragraph 50\n","Style: Heading 2\n","Heading: Heading 2\n","Text: Smart Vision – Brane Enterprises\n","\n","Paragraph 51\n","Style: List Bullet\n","Text: We created a smart eyewear prototype equipped with an integrated camera to aid visually impaired individuals. Our in-house fabrication of the PCB board enables essential functionalities like Bluetooth\n","\n","Paragraph 52\n","Style: List Bullet\n","Text: Techniques Used: Faster-RCNN, MobileNetV2, CNN+LSTM, YOLO\n","\n","Paragraph 53\n","Style: Heading 2\n","Heading: Heading 2\n","Text: Incremental Face Learning – brane enterprises\n","\n","Paragraph 54\n","Style: List Bullet\n","Text: We developed an Android application for conducting face recognition using transfer learning techniques. We utilized the FaceNet model as the backbone network and added a classification layer for face \n","\n","Paragraph 55\n","Style: List Bullet\n","Text: Techniques Used: Transfer Learning, FaceNet, Incremental Learning\n","\n","Paragraph 56\n","Style: Heading 2\n","Heading: Heading 2\n","Text: Action Recognition – Montpellier university\n","\n","Paragraph 57\n","Style: List Bullet\n","Text: Given one second long measure of the human’s motion, the system can determine human action. The originality lies in the use of joint angles, instead of cartesian coordinates. This design choice makes \n","\n","Paragraph 58\n","Style: List Bullet\n","Text: Techniques Used: Bi-LSTM, Mocap, OpenPose, CNN\n","\n","Paragraph 59\n","Style: Heading 2\n","Heading: Heading 2\n","Text: Visual Grounding – Umea university\n","\n","Paragraph 60\n","Style: List Bullet\n","Text: For robots to engage with humans in real-world situations or with objects, they must develop a mental representation (\"state of mind\") that a) reflects the robots' perception and b) ideally aligns wit\n","\n","Paragraph 61\n","Style: List Bullet\n","Text: Techniques Used: Fuzzy Inference System, Mask-RCNN, CNN, Local Binary pattern (LBP), Multi-Layer Perceptron\n","\n","Paragraph 62\n","Style: Heading 1\n","Heading: Heading 1\n","Text: Talks and Presentations\n","\n","Paragraph 63\n","Style: List Paragraph\n","Text: Title: An empirical review of calibration techniques for the Pepper humanoid robot’s RGB and depth camera.\n","Venue: Intelligent systems and application, 5th Sep 2019, London, England.\n","Occasion: Presente\n","\n","Paragraph 64\n","Style: List Paragraph\n","Text: Fusion of gesture and speech for increased accuracy in human robot interaction.\n","Venue: 25th International conference on methods and models in automation and robotics, 24th Aug 2019, Międzyzdroje, Pola\n","\n","Paragraph 65\n","Style: List Paragraph\n","Text: Conflict Detection and Resolution in Table Top Scenarios for Human-Robot Interaction.\n","Venue: Computing science department, Umea University, 18th Jun, 2019, Umea, Sweden.\n","Occasion: Poster presentation \n","\n","Paragraph 66\n","Style: List Paragraph\n","Text: Deep learning and its applications.\n","Venue: UFBI department, Umea University, 15th Jun 2018, Umea, Sweden.\n","Occasion: Invited as a speaker at Umea center for Functional Brain Imaging (UFBI) day.\n","\n","Paragraph 67\n","Style: List Paragraph\n","Text: Sketch drawing by NAO humanoid robot.\n","Venue: TENCON a premier international technical conference of IEEE Region 10, 1st Nov, 2015, Macau, China.\n","Occasion: Presented conference paper in TENCON 2015.\n","\n","Paragraph 68\n","Style: Heading 1\n","Heading: Heading 1\n","Text: Selected Publications [ Journals and Conferences]\n","\n","Paragraph 69\n","Style: Heading 2\n","Heading: Heading 2\n","Text: journal publications\n","\n","Paragraph 70\n","Style: List Bullet\n","Text: Singh, A. K., Baranwal, N., Richter, K. F., Hellström, T., & Bensch, S. (2020). Verbal explanations by collaborating robot teams. Paladyn, Journal of Behavioral Robotics, 12(1), 47-57.\n","\n","Paragraph 71\n","Style: List Bullet\n","Text: Singh, A. K., Baranwal, N., & Nandi, G. C. (2019). A rough set based reasoning approach for criminal identification. International Journal of Machine Learning and Cybernetics, 10, 413-431.\n","\n","Paragraph 72\n","Style: List Bullet\n","Text: Baranwal, N., Nandi, G. C., & Singh, A. K. (2017). Real‐Time Gesture–Based Communication Using Possibility Theory–Based Hidden Markov Model. Computational Intelligence, 33(4), 843-862.\n","\n","Paragraph 73\n","Style: List Bullet\n","Text: Baranwal, N., Singh, A. K., & Nandi, G. C. (2017). Development of a framework for human–robot interactions with Indian sign language using possibility theory. International Journal of Social Robotics,\n","\n","Paragraph 74\n","Style: List Bullet\n","Text: Singh, A. K., Baranwal, N., & Nandi, G. C. (2017). Development of a self reliant humanoid robot for sketch drawing. Multimedia Tools and Applications, 76, 18847-18870.\n","\n","Paragraph 75\n","Style: Heading 2\n","Heading: Heading 2\n","Text: conference publications\n","\n","Paragraph 76\n","Style: List Bullet\n","Text: Singh, A. K., Adjel, M., Bonnet, V., Passama, R., & Cherubini, A. (2022, November). A framework for recognizing industrial actions via joint angles. In 2022 IEEE-RAS 21st International Conference on H\n","\n","Paragraph 77\n","Style: List Bullet\n","Text: Kumar Singh, A., Baranwal, N., & Richter, K. F. (2020). A fuzzy inference system for a visually grounded robot state of mind. In ECAI 2020 (pp. 2402-2409). IOS Press.\n","\n","Paragraph 78\n","Style: List Bullet\n","Text: Singh, A. K., Baranwal, N., & Richter, K. F. (2020). An empirical review of calibration techniques for the pepper humanoid robot’s RGB and depth camera. In Intelligent Systems and Applications: Procee\n","\n","Paragraph 79\n","Style: List Bullet\n","Text: Singh, A. K., Chakraborty, P., & Nandi, G. C. (2015, November). Sketch drawing by nao humanoid robot. In TENCON 2015-2015 IEEE Region 10 Conference (pp. 1-6). IEEE.\n","\n","Paragraph 81\n","Style: List Bullet\n","Text: Please find the complete list of publication at my Google Scholar Profile.\n","\n","Paragraph 82\n","Style: Heading 1\n","Heading: Heading 1\n","Text: Online Presence\n","\n","Paragraph 83\n","Style: Normal\n","Text: Github: https://github.com/robaita\n","\n","Paragraph 84\n","Style: Normal\n","Text: LinkedIn: https://fr.linkedin.com/in/dr-avinash-kumar-singh-2a570a31\n"]}]},{"cell_type":"markdown","source":["# Reading Excel/CSV files"],"metadata":{"id":"ja5Im_rzrzxo"}},{"cell_type":"code","source":["excel_loader = UnstructuredExcelLoader(\"/content/drive/MyDrive/Zeta Workshop/dataset/headcount_2025.xlsx\")\n","excel_docs = excel_loader.load()\n","excel_docs[0].page_content[:200]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"9g_yYFBzr2Qq","executionInfo":{"status":"ok","timestamp":1748451458041,"user_tz":-330,"elapsed":1195,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"c8db63cc-a2c4-4b87-96db-467c8d207b37"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Name Department Position Salary (INR) 2025 Increase Sarah Verma Finance Financial Analyst 899000 11.78% Aditya Verma Sales Sales Manager 772000 10.18% Vihaan Gupta Marketing Digital Marketer 753000 3.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Load Excel file\n","import pandas as pd\n","file_path = \"/content/drive/MyDrive/Zeta Workshop/dataset/headcount_2025.xlsx\"\n","xls = pd.ExcelFile(file_path)\n","\n","# List all sheets\n","print(\"Sheets:\", xls.sheet_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lwDeDR5U0k2y","executionInfo":{"status":"ok","timestamp":1748451489326,"user_tz":-330,"elapsed":325,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"7a103bfc-ef81-4743-ab1d-cfd65e52a5d5"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Sheets: ['Sheet1']\n"]}]},{"cell_type":"code","source":["# Read specific sheet (or loop through all)\n","for sheet in xls.sheet_names:\n","    print(f\"\\n--- Sheet: {sheet} ---\")\n","    df = xls.parse(sheet)\n","    print(df.head())  # Show first few rows\n","\n","    # Optionally convert each table to a dict or JSON for later use\n","    table_as_dict = df.to_dict(orient=\"records\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNCcyafK1IIX","executionInfo":{"status":"ok","timestamp":1748451501790,"user_tz":-330,"elapsed":317,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"d5319c1b-d51b-49d5-da23-ebe115703913"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- Sheet: Sheet1 ---\n","           Name Department             Position  Salary (INR) 2025 Increase\n","0   Sarah Verma    Finance    Financial Analyst        899000        11.78%\n","1  Aditya Verma      Sales        Sales Manager        772000        10.18%\n","2  Vihaan Gupta  Marketing     Digital Marketer        753000          3.7%\n","3  Kiara Mishra      Legal        Legal Manager        753000        13.39%\n","4  Atharv Gupta  Marketing  Marketing Executive       1116000         3.25%\n"]}]},{"cell_type":"markdown","source":["# Reading Webpages"],"metadata":{"id":"AgNIOXuwr5u4"}},{"cell_type":"code","source":["web_loader = WebBaseLoader(\"https://robaita.com/about/\")\n","web_docs = web_loader.load()"],"metadata":{"id":"UiysGQdSr72N","executionInfo":{"status":"ok","timestamp":1748452176752,"user_tz":-330,"elapsed":666,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["web_docs[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r5fRFmU41TH3","executionInfo":{"status":"ok","timestamp":1748452177942,"user_tz":-330,"elapsed":18,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"cf24b293-dd83-4e37-a65b-f43188c36bcc"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(metadata={'source': 'https://robaita.com/about/', 'title': 'About | Robaita', 'description': 'Hi there! Thank you so much for visiting our page. We are Robaita, which stands for Robotics and Artificial Intelligence Training Academy…', 'language': 'en'}, page_content=\"About | RobaitaSkip to contentRobaitaBlogAboutCoursesGithubHomepageLinkedInAboutHi there!\\nThank you so much for visiting our page. We are Robaita, which stands for Robotics and Artificial Intelligence Training Academy. Our website is designed to spread awareness about robotics and artificial intelligence. Our mission is to support and uplift underprivileged students and aspiring individuals who are interested in these fields. We are passionate about open source and teaching. All of our source codes, blogs, and materials are freely available for anyone to use. Let's learn and grow together. \\nॐ सह नाववतु । \\nसह नौ भुनक्तु । \\nसह वीर्यं करवावहै । \\nतेजस्वि नावधीतमस्तु मा विद्विषावहै । \\nॐ शान्तिः शान्तिः शान्तिः ॥© 2024 by Robaita. All rights reserved.Theme by LekoArts\\n\\n\")"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["print(web_docs[0].metadata['source'])\n","print(web_docs[0].metadata['title'])\n","print(web_docs[0].metadata['description'])\n","print(web_docs[0].page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mIKmaA4c2C0g","executionInfo":{"status":"ok","timestamp":1748452180311,"user_tz":-330,"elapsed":16,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"ee3f07f2-7fb5-412a-aef2-2f926cb24d2d"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["https://robaita.com/about/\n","About | Robaita\n","Hi there! Thank you so much for visiting our page. We are Robaita, which stands for Robotics and Artificial Intelligence Training Academy…\n","About | RobaitaSkip to contentRobaitaBlogAboutCoursesGithubHomepageLinkedInAboutHi there!\n","Thank you so much for visiting our page. We are Robaita, which stands for Robotics and Artificial Intelligence Training Academy. Our website is designed to spread awareness about robotics and artificial intelligence. Our mission is to support and uplift underprivileged students and aspiring individuals who are interested in these fields. We are passionate about open source and teaching. All of our source codes, blogs, and materials are freely available for anyone to use. Let's learn and grow together. \n","ॐ सह नाववतु । \n","सह नौ भुनक्तु । \n","सह वीर्यं करवावहै । \n","तेजस्वि नावधीतमस्तु मा विद्विषावहै । \n","ॐ शान्तिः शान्तिः शान्तिः ॥© 2024 by Robaita. All rights reserved.Theme by LekoArts\n","\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vD1uJjO33iJ3","executionInfo":{"status":"ok","timestamp":1748452204488,"user_tz":-330,"elapsed":45,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}}},"execution_count":49,"outputs":[]}]}