{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOdFhFv+ux+YN3ID5HGX/Jh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7UG2Z1Z83-ea","executionInfo":{"status":"ok","timestamp":1748453071916,"user_tz":-330,"elapsed":179090,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"a26e8681-6edd-4810-b58b-e241b189df56"},"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n","Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,702 kB]\n","Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n","Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,735 kB]\n","Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,951 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,564 kB]\n","Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,987 kB]\n","Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n","Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,402 kB]\n","Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [34.3 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,264 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,552 kB]\n","Fetched 31.9 MB in 7s (4,571 kB/s)\n","Reading package lists... Done\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  poppler-utils\n","0 upgraded, 1 newly installed, 0 to remove and 63 not upgraded.\n","Need to get 186 kB of archives.\n","After this operation, 697 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.8 [186 kB]\n","Fetched 186 kB in 0s (439 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package poppler-utils.\n","(Reading database ... 126109 files and directories currently installed.)\n","Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.8_amd64.deb ...\n","Unpacking poppler-utils (22.02.0-2ubuntu0.8) ...\n","Setting up poppler-utils (22.02.0-2ubuntu0.8) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Collecting unstructured-pytesseract\n","  Downloading unstructured.pytesseract-0.3.15-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from unstructured-pytesseract) (24.2)\n","Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-pytesseract) (11.2.1)\n","Downloading unstructured.pytesseract-0.3.15-py3-none-any.whl (14 kB)\n","Installing collected packages: unstructured-pytesseract\n","Successfully installed unstructured-pytesseract-0.3.15\n","Collecting python-docx\n","  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.2)\n","Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-docx\n","Successfully installed python-docx-1.1.2\n","Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n","Collecting langchain-community\n","  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n","Collecting unstructured\n","  Downloading unstructured-0.17.2-py3-none-any.whl.metadata (24 kB)\n","Collecting pdf2image\n","  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n","Collecting pytesseract\n","  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n","Collecting pdfminer.six\n","  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n","Collecting pi_heif\n","  Downloading pi_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n","Collecting unstructured_inference\n","  Downloading unstructured_inference-1.0.2-py3-none-any.whl.metadata (5.3 kB)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.60)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n","Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n","  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n","Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.2.0)\n","Collecting filetype (from unstructured)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting python-magic (from unstructured)\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.9.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.4)\n","Collecting emoji (from unstructured)\n","  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n","Collecting python-iso639 (from unstructured)\n","  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n","Collecting langdetect (from unstructured)\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting rapidfuzz (from unstructured)\n","  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting backoff (from unstructured)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.2)\n","Collecting unstructured-client (from unstructured)\n","  Downloading unstructured_client-0.35.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.17.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.9.5)\n","Collecting python-oxmsg (from unstructured)\n","  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n","Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pdf2image) (11.2.1)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.2)\n","Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n","Collecting python-multipart (from unstructured_inference)\n","  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (0.31.4)\n","Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (4.11.0.86)\n","Collecting onnx (from unstructured_inference)\n","  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n","Collecting onnxruntime>=1.18.0 (from unstructured_inference)\n","  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (3.10.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (2.6.0+cu124)\n","Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (1.0.15)\n","Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (4.52.2)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (1.7.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from unstructured_inference) (1.15.3)\n","Collecting pypdfium2 (from unstructured_inference)\n","  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n","Collecting coloredlogs (from onnxruntime>=1.18.0->unstructured_inference)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.18.0->unstructured_inference) (25.2.10)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.18.0->unstructured_inference) (5.29.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.18.0->unstructured_inference) (1.13.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n","  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured_inference) (3.18.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured_inference) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured_inference) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured_inference) (0.5.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->unstructured_inference) (2025.3.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->unstructured_inference)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->unstructured_inference)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->unstructured_inference)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->unstructured_inference)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->unstructured_inference)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->unstructured_inference)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->unstructured_inference)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->unstructured_inference)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->unstructured_inference)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->unstructured_inference)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured_inference) (3.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.18.0->unstructured_inference) (1.3.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured) (2.7)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (1.17.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (0.5.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured_inference) (2.9.0.post0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (1.5.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured_inference) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured_inference) (2025.2)\n","Collecting olefile (from python-oxmsg->unstructured)\n","  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm->unstructured_inference) (0.21.0+cu124)\n","Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n","  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.6.0)\n","Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n","  Downloading pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.18.0->unstructured_inference)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->unstructured_inference) (3.0.2)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n","Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured-0.17.2-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n","Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n","Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pi_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured_inference-1.0.2-py3-none-any.whl (47 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n","Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n","Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unstructured_client-0.35.0-py3-none-any.whl (192 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m192.1/192.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n","Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=f87a71321907a0cbf5d379da5621475325b33c8a1ae88d8b6c2f8042e236d8b4\n","  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n","Successfully built langdetect\n","Installing collected packages: filetype, rapidfuzz, python-multipart, python-magic, python-iso639, python-dotenv, pytesseract, pypdfium2, pypdf, pi_heif, pdf2image, onnx, olefile, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, langdetect, humanfriendly, httpx-sse, emoji, backoff, aiofiles, typing-inspect, python-oxmsg, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, unstructured-client, pydantic-settings, pdfminer.six, onnxruntime, nvidia-cusolver-cu12, dataclasses-json, unstructured, unstructured_inference, langchain-community\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed aiofiles-24.1.0 backoff-2.2.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 emoji-2.14.1 filetype-1.2.0 httpx-sse-0.4.0 humanfriendly-10.0 langchain-community-0.3.24 langdetect-1.0.9 marshmallow-3.26.1 mypy-extensions-1.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 olefile-0.47 onnx-1.18.0 onnxruntime-1.22.0 pdf2image-1.17.0 pdfminer.six-20250506 pi_heif-0.22.0 pydantic-settings-2.9.1 pypdf-5.5.0 pypdfium2-4.30.1 pytesseract-0.3.13 python-dotenv-1.1.0 python-iso639-2025.2.18 python-magic-0.4.27 python-multipart-0.0.20 python-oxmsg-0.0.2 rapidfuzz-3.13.0 typing-inspect-0.9.0 unstructured-0.17.2 unstructured-client-0.35.0 unstructured_inference-1.0.2\n"]}],"source":["!sudo apt-get update\n","!sudo apt-get install -y poppler-utils\n","\n","!pip install -U unstructured-pytesseract\n","!pip install python-docx\n","\n","!pip install -U langchain langchain-community unstructured pdf2image pytesseract python-docx openpyxl pdfminer.six pi_heif unstructured_inference"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZuiRJMLu59mD","executionInfo":{"status":"ok","timestamp":1748453093488,"user_tz":-330,"elapsed":21560,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"2ee721b5-7a12-4732-95b9-d77405b0b11b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# RAG Pipeline: Chunking and Indexing with LangChain and LlamaIndex\n","\n","# ğŸ“˜ What is Chunking in RAG?\n","\n"," **Chunking** is the process of splitting a large document into smaller, semantically meaningful parts or \"chunks\". These chunks are then embedded and indexed, allowing the RAG system to retrieve the most relevant information to answer a user query.\n","\n","# ğŸ’¡ Importance of Chunking\n"," - **Improves retrieval accuracy**\n"," - **Fits model context window**\n"," - **Preserves semantic meaning**\n"," - **Supports efficient embedding and indexing**"],"metadata":{"id":"EQu6GNFm5Cgc"}},{"cell_type":"code","source":["from langchain.document_loaders import (\n","    UnstructuredPDFLoader,\n","    UnstructuredWordDocumentLoader,\n","    UnstructuredExcelLoader,\n","    WebBaseLoader\n",")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a5BAQvSP5t2Q","executionInfo":{"status":"ok","timestamp":1748453095780,"user_tz":-330,"elapsed":2290,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"cd4da563-39d6-480a-8a8e-d6902a181f4e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"]}]},{"cell_type":"code","source":["pdf_loader = UnstructuredPDFLoader(\"/content/drive/MyDrive/Zeta Workshop/dataset/Avinash-CV.pdf\", mode=\"elements\")\n","elements = pdf_loader.load()\n","\n","page_data = {}\n","for idx, el in enumerate(elements):  # First 5 elements\n","    # print(f\"\\n--- Page {idx+1} ---\")\n","    # print(\"Available Metadata Keys:\", el.metadata.keys())\n","    # print(\"Metadata:\", el.metadata)\n","    # print(\"Page Number:\", el.metadata['page_number'])\n","    # print(\"Text:\", el.page_content)\n","    if el.metadata['page_number'] not in page_data:\n","      page_data[el.metadata['page_number']] = [el.page_content]\n","    else:\n","      page_data[el.metadata['page_number']].append(el.page_content)"],"metadata":{"id":"AEt771WB6MY5","executionInfo":{"status":"ok","timestamp":1748453328492,"user_tz":-330,"elapsed":55468,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["for page in page_data:\n","  print(f\"Page {page}:\")\n","  print(page_data[page])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5d3iMFXw6R-n","executionInfo":{"status":"ok","timestamp":1748453334952,"user_tz":-330,"elapsed":90,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"48487445-02e0-41ac-9a51-dbeba595b5b7"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Page 1:\n","['Dr. Avinash Kumar Singh', 'Hyderabad, India | +91-9005722861 | avinashkumarsingh1986@gmail.com | http://avinashkumarsingh.in', 'Profile', 'With over 14 years in Al, I have evolved through roles as an ML Researcher, Engineer, Product Manager, and now as Chief Al Scientist. | have led the development and deployment of deep learning-based computer vision and NLP models on platforms like AWS, GCP, Humanoid Robots, Edge Devices like Jetson Nano, Raspberry Pi, and NXP boards. My expertise extends to tackling challenges in concurrency, security, and latency. My academic journey, enriched by a Ph.D. and postdoctoral research, provides a profound understanding of neural networks across diverse data types, while my industrial experience ensures practical AI solutions are deployed effectively, serving real users. This unique blend of research and industry expertise enables me to lead in crafting and delivering impactful Al innovations, driving business transformation and societal advancement.', 'Experience AI CONSULTANT & CORPORATE TRAINER | ROBAITA, HYDERABAD, INDIA | SEP 24 - CONT...', '= Mentored and empowered 1,500+ students and working professionals from diverse backgrounds. Led immersive sessions on Fine-tuning Large Language Models (LLMs), designing robust Retrieval-Augmented Generation (RAG) systems, building custom AI chatbots, defending LLMs against prompt injection attacks, implementing Model Context Protocol (MCP) for efficient context management, and developing Agentic Al solutions.', '= Engineered and deployed a cutting-edge visual language model-based computer vision system to drastically reduce loose picking of Apple iPhones ina warehouse environment, achieving a significant reduction from 20% to 5%. This innovative solution leverages visual question answering to enhance product identification, ensuring operational efficiency and accuracy.', '= Designed and developed an advanced conversational AI system tailored to resolve complex challenges within the supply chain. This intelligent system facilitates seamless communication with each supply chain node and accurately predicts arrival times by analyzing routes and external factors such as weather disruptions, protests, and regulatory conditions, ensuring timely and compliant operations.', '= Conceptualized and implemented \"Talk to Your Document,\" a generative Al solution that enables interactive question-and-answer sessions with historical documents and databases. This solution achieved impressive BLEU and ROUGE scores of 0.85, demonstrating exceptional accuracy in retrieving and synthesizing relevant information.', 'GLOBAL SOLUTION LEADER | BRANE ENTERPRISES LLP, HYDERABAD, INDIA | MAY 20 - SEP 24', '= A LLM model is finetuned on financial reports to understand complex financial reasoning (mostly tables). The system could generate the mathematical formula to process the query, retrieve the argument from the table and compute the value as the output.', '= Implemented a Face Recognition-based office attendance system, replacing the existing RFID system, and achieving organization-wide deployment.', '= The system serves 2,856 employees with a 97.63% accuracy rate, resulting in annual savings in operational costs.', '= Leda groundbreaking project to design smart glasses for visually impaired individuals, providing comprehensive assistance in reading, navigation, currency identification, person recognition, and scene understanding.', '= The system can detect obstacle up to 5 feet, can help in reading English and six Indian languages, could recognize 9,605 objects and labels with 89.76% accuracy.', '= Successfully delivered a Driver Monitoring System (DMS), incorporating real-time monitoring and safety features. The system tracks driver drowsiness, smoking, drinking, eating, phone usage, and seatbelt compliance, resulting in a 40% reduction in driving violations.', '= Captured and recorded over 1,000 violations with images, date, time, and other details.', '= Provided live streams from both interior and exterior dash cameras, enhancing monitoring accuracy.']\n","Page 2:\n","['SENIOR RESEARCHER | MONTPELLIER UNIVERSITY, FRANCE | NOV 20- NOV 21', '= | was associated with the robotics lab (LIRMM) and worked on the EU Project SOPHIA. As the in-charge of Work package 5, I helped my team to coordinate between different project partners Italian Institute of Technology, Italy, INAIL, Italy, and LIRMM, France for data acquisition, human-robot interaction and to derive a deep learning model', 'for action recognition.', '= Designed and developed a sensor agnostic, Bidirectional LSTM based deep neural network for action recognition. The model is tested in the presence of Xsens suit (used for motion capture) and Intel RealSense and Microsoft Kinect RGB-D data (3D skeleton). The research is published in 21st International Conference on Humanoid Robots.', '= The model is integrated with KUKA robot to help human in physical assistance, e.g. carrying object, release object, place object etc. in industrial environment by understanding the human actions.', 'POST DOCTORAL RESEARCHER | UMEA UNIVERSITY, SWEDEN | FEB 18 - JAN 20', '= During this postdoc, | closely worked with Professor Kai-Florian Richter and Professor Thomas HellstrÃ©m. | was a part of intelligent robotics lab, during this postdoc, we designed a dialogue based human robot interaction system that allows humans, to talk to the robot. This work was published in a â€˜Aâ€™ rated conference ECAI-2020.', '= We developed and implemented a robot collaboration framework that enables robots to have dialogues by translating their actions into the natural language. This work was published in Journal of behavioural robotics and also featured in Softbank robotics under the best 20 projects in 2020.', 'DEPUTY MANAGER | HCL TECHNOLOGIES, NOIDA, INDIA | FEB 17 - JAN 18', '= | joined the HCL machine learning division (Noida) when this was a 3 members team. In the span of one year, we conducted 4 successful POCs and grew the team to a 16 members team.', '= Asa deputy manager my responsibilities were to handle the client interaction, project scoping, find the place where machine learning solutions can be pitched (to integrated with existing workflows) within and outside the organization.', '= Helped my team to set up the machine learning GPU infrastructure, sketching project roadmaps, resource allocation and tracking. Further motivating team to follow the software engineering best practices such as maintaining git, hygiene of code (following the coding standards), etc.', 'ML ENGINEER | ECLERX SERVICES LIMITED, MUMBAI, INDIA | NOV 15 - FEB 17', '= eClerx is an Indian IT consulting and outsourcing multinational company. I worked there as a full stack developer and my job was to design and develop NLP, ML solutions. We used image pre-processing to improve OCR accuracy, further to integrate these solutions with RPA system.', 'Education PH.D | COMPUTER VISION | DEC 2016] IIIT, ALLAHABAD, INDIA', 'M.TECH | INFORMATION SECURITY | JUNE 2011 | KIIT UNIVERSITY, BHUBANESHWAR, INDIA M.SC | INFORMATION TECHNOLOGY | JUNE 2009 | KUMAUN UNIVERSITY, ALMORA, INDIA', 'B.SC | MATHEMATICS | JUNE 2007 | KUMAUN UNIVERSITY, ALMORA, INDIA', 'Skills & Abilities', '= Machine Learning = Machine learning operations (MLOps) = Computer vision = Certified scrum master', '= Natural language processing = AWS, GCP, E2E cloud platforms', '= Generative Al, LLM, Vision Language Model = Python, TensorFlow, Pytorch, MySQL', 'Page 2']\n","Page 3:\n","['= Genetic Algorithms, Fuzzy Systems = Human-Robot Interactions (Nao, Pepper)', 'Certifications', '= Machine Learning Operations (MLOps) Coursera - Deeplearnining.ai Focus Areas - ML Workflows, TFX, Model Deployments and Tracking for Production May 3, 2022 = TensorFlow 2 for Deep Learning Coursera - Imperial College London Focus Areas â€” TensorFlow 2 APIs, Customized Model Training, Probabilistic Deep Learning Jul 9, 2023 = Machine Learning and Soft Computing Indian Statistical Institute, Kolkata, India Focus Areas - Artificial Neural Network, Computer Vision and Genetic Algorithms Dec 20, 2012', 'Achievements', '= SOPHIA - A H2020 EU PROJECT .', 'Secured three years post-doctoral position on deep learning perception for human-robot collaboration at LIRMM Montpellier.', '= POST-DOCTORAL FUNDING FROM KEMPHE .', 'FOUNDATIONS, SWEDEN Received 2 years of research funding to pursue my postdoctoral work in Human-Robot Interaction at Umea University, Sweden.', '= 1ST POSITION IN M.SC Secured first position in M.Sc for consecutive 2 years at university level.', 'Projects', 'Git, CI/CD, Dockers, Micro Services (REST), JIRA 10T - ESP32, MQTT, Jetson Nano, Raspberry, NXP', 'Building Cloud Computing Solutions at Scale Coursera - Duke University', 'Focus Areas - Containers, Cloud APIs, DevOps, AWS an Azure', 'Nov 6, 2023', 'Scrum Master Certification Specialization Coursera â€” Learn Quest', 'Focus Areas - Agile Methodology and Product Management', 'Feb 17, 2023', 'Prompt Engineering for ChatGPT', 'Vanderbilt University', 'Focus Area - Few Shot Examples, Meta Language Creation Pattern, Different prompts patterns July 9, 2023', 'MARIE SKLODOWSKA CURIE ACTIONS (MSCA) 2020 Applied for the MSCA post-doctoral funding in association with Universitat Rovira i Virgili, Spain and received 85% marks.', 'MHRD, INDIA PH.D FELLOWSHIP', 'Received 2 years of junior research and 2 years of senior research funding from MHRD for the Ph.D program under robotics and AI lab, IIIT Allahabad.', 'FINETUNENING OF LLM FOR Q&A ON WEBSITE DATA', \"In today's world, simply having an FAQ section or static information on a website isn't enough. We can harness this data to build a conversational Al system that provides more effective and intuitive search and information retrieval. To tackle this, we fine-tuned the LLaMA model using data scraped from the website. Initially, we generated question- answer pairs using the website content and ChatGPT. Once we had sufficient data, we used it to fine-tune the model. The model was then deployed as a conversational Al system using OpenWebUI, enabling users to engage with the content in a more interactive manner.\", 'Techniques Used: Llama, OpenWebUI, LoRA, Data Preparation, Tokenization and Encoding.', 'Page 3']\n","Page 4:\n","['FINANCE LARGE LANGUAGE MODEL', 'We fine-tuned BERT, an LLM model, to handle complex reasoning tasks found in financial reports. We utilized the FinQA dataset, which consists of earnings reports from S&P 500 companies spanning from 1999 to 2019. These reports, typically in PDF format, include multiple pages of financial data, often presented in tables and text. The model generates the mathematical equation necessary to conduct the calculation and further execute that equation on table selected values to get the output value (execution value). The model achieved an execution accuracy of 65.05%.', 'Techniques Used: BERT, Language Embedding, Program Accuracy, Execution Accuracy', 'SMART VISION - BRANE ENTERPRISES', 'We created a smart eyewear prototype equipped with an integrated camera to aid visually impaired individuals. Our in-house fabrication of the PCB board enables essential functionalities like Bluetooth connectivity, USB charging, and live-streaming to an Android device. Additionally, we developed a companion app providing features such as object detection, scene analysis, navigation assistance, and person counting. My role involved collaborating with the team to develop and integrate deep learning solutions into the app. Together, we successfully implemented multiple models for object detection, image/dense captioning, currency recognition, OCR, and more.', 'Techniques Used: Faster-RCNN, MobileNetV2, CNN+LSTM, YOLO', 'INCREMENTAL FACE LEARNING - BRANE ENTERPRISES', 'We developed an Android application for conducting face recognition using transfer learning techniques. We utilized the FaceNet model as the backbone network and added a classification layer for face classification. One challenge with neural networks is catastrophic forgetting, where training the entire network (excluding the backbone) on N+1 classes can lead to performance issues. To address this, we adopted an incremental learning approach that helps to train and deploy the model in 30 seconds reducing the onboarding time.', 'Techniques Used: Transfer Learning, FaceNet, Incremental Learning', 'ACTION RECOGNITION - MONTPELLIER UNIVERSITY', 'Given one second long measure of the humanâ€™s motion, the system can determine human action. The originality lies in the use of joint angles, instead of cartesian coordinates. This design choice makes the framework sensor agnostic and invariant to affine transformations and to anthropometric differences. On AnDy dataset, we outperform the state of the art classifier. Furthermore, we show that our system is effective with limited training data, that it is subject independent, and that it is compatible with robotic real time constraints. In terms of methodology, the system is an original synergy of two antithetical schools of thought: model based and data-based algorithms. Indeed, it is the cascade of an inverse kinematics estimator compliant with the International Society of Biomechanics recommendations, followed by a deep learning architecture based on Bidirectional Long Short Term Memory. Techniques Used: Bi-LSTM, Mocap, OpenPose, CNN', 'VISUAL GROUNDING - UMEA UNIVERSITY', 'For robots to engage with humans in real-world situations or with objects, they must develop a mental representation (\"state of mind\") that a) reflects the robotsâ€™ perception and b) ideally aligns with human comprehension and ideas. Using table-top scenarios as an example, we propose a framework for generating a robot\\'s \"state of mindâ€ by identifying the objects on the table along with their characteristics (color, shape, texture) and spatial relationships to one another. The robot\\'s view of the scene is depicted in a dynamic graph where object attributes are translated into fuzzy linguistic variables that correspond to human spatial concepts. This endeavor involves creating these graph representations through a combination of low-level neural network-based feature recognition and a high-level fuzzy inference system.', 'Techniques Used: Fuzzy Inference System, Mask-RCNN, CNN, Local Binary pattern (LBP), Multi-Layer Perceptron', 'Page 4']\n","Page 5:\n","['Talks and Presentations', '= Title: An empirical review of calibration techniques for the Pepper humanoid robotâ€™s RGB and depth camera. Venue: Intelligent systems and application, 5â€˜ Sep 2019, London, England. Occasion: Presented conference paper in IntelliSys 2019. = Fusion of gesture and speech for increased accuracy in human robot interaction. Venue: 25th International conference on methods and models in automation and robotics, 24 Aug 2019, Miedzyzdroje, Poland. Occasion: Presented conference paper in MMAR 2019. = Conflict Detection and Resolution in Table Top Scenarios for Human-Robot Interaction. Venue: Computing science department, Umea University, 18 Jun, 2019, Umea, Sweden. Occasion: Poster presentation in 31st Swedish Al Society Workshop. = Deep learning and its applications. Venue: UFBI department, Umea University, 15\" Jun 2018, Umea, Sweden. Occasion: Invited as a speaker at Umea center for Functional Brain Imaging (UFBI) day. = Sketch drawing by NAO humanoid robot. Venue: TENCON a premier international technical conference of IEEE Region 10, 1st Nov, 2015, Macau, China. Occasion: Presented conference paper in TENCON 2015.', 'Selected Publications [ Journals and Conferences]', 'JOURNAL PUBLICATIONS', '= Singh, A. K., Baranwal, N., Richter, K. F., Hellstrom, T., & Bensch, S. (2020). Verbal explanations by collaborating robot teams. Paladyn, Journal of Behavioral Robotics, 12(1), 47-57.', '= Singh, A. K., Baranwal, N., & Nandi, G. C. (2019). A rough set based reasoning approach for criminal identification. International Journal of Machine Learning and Cybernetics, 10, 413-431.', '= Baranwal, N., Nandi, G.C., & Singh, A. K. (2017). Real-Time Gesture-Based Communication Using Possibility Theory- Based Hidden Markov Model. Computational Intelligence, 33(4), 843-862.', '= Baranwal, N., Singh, A. K., & Nandi, G. C. (2017). Development of a framework for human-robot interactions with Indian sign language using possibility theory. International Journal of Social Robotics, 9, 563-574.', '= Singh, A. K., Baranwal, N., & Nandi, G. C. (2017). Development of a self reliant humanoid robot for sketch drawing. Multimedia Tools and Applications, 76, 18847-18870.', 'CONFERENCE PUBLICATIONS', '= Singh, A. K.,, Adjel, M., Bonnet, V., Passama, R., & Cherubini, A. (2022, November). A framework for recognizing industrial actions via joint angles. In 2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids) (pp. 210-216). IEEE.', '= Kumar Singh, A., Baranwal, N., & Richter, K. F. (2020). A fuzzy inference system for a visually grounded robot state of mind. In ECAI 2020 (pp. 2402-2409). IOS Press.', '= Singh, A. K., Baranwal, N., & Richter, K. F. (2020). An empirical review of calibration techniques for the pepper humanoid robotâ€™s RGB and depth camera. In Intelligent Systems and Applications: Proceedings of the 2019 Intelligent Systems Conference (IntelliSys) Volume 2 (pp. 1026-1038). Springer International Publishing.', '= Singh, A. K., Chakraborty, P., & Nandi, G. C. (2015, November). Sketch drawing by nao humanoid robot. In TENCON 2015-2015 IEEE Region 10 Conference (pp. 1-6). IEEE.', 'Please find the complete list of publication at my Google Scholar Profile.', 'Online Presence', 'Github: https://github.com/robaita LinkedIn: https://fr.linkedin.com/in/dr-avinash-kumar-singh-2a570a31', 'Page 5']\n"]}]},{"cell_type":"markdown","source":["# ğŸ§± Chunking Methodologies\n","\n","| Method | Definition | How it Works |\n","|--------|------------|--------------|\n","| Fixed-size | Breaks text into equal-sized blocks | Splits by a predefined character/token limit |\n","| Semantic | Breaks text at logical boundaries | Uses sentence or paragraph segmentation |\n","| Recursive | Tries semantic split first, falls back to smaller units | Combines semantic and fixed approaches |\n","| Sliding Window | Uses overlapping chunks | Ensures context flow across adjacent chunks |\n"],"metadata":{"id":"pLzxdvgE6VRm"}},{"cell_type":"markdown","source":["### ğŸ”¹ Fixed-size Chunking\n","\n","**Definition:** Splits text into equal-sized segments.\n","\n","**How it Works:** You define `chunk_size` and `chunk_overlap`. It doesnâ€™t consider sentence boundaries.\n"],"metadata":{"id":"L5I_2cck6nK5"}},{"cell_type":"code","source":["from langchain.text_splitter import CharacterTextSplitter\n","\n","text = \"\"\n","for page in page_data:\n","  text = text + \"\\n\".join(page_data[page])\n","# print(text)\n","\n","splitter = CharacterTextSplitter(\n","    chunk_size=200,\n","    chunk_overlap=50,\n","    separator=\"\"  # Force strict character-level splitting\n",")\n","chunks = splitter.split_text(text)\n","\n","for i, chunk in enumerate(chunks):\n","    print(f\"Chunk {i+1}: {repr(chunk)}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"njoPn3xq6juV","executionInfo":{"status":"ok","timestamp":1748454514720,"user_tz":-330,"elapsed":99,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"e850d963-d06f-49d9-d496-d7b72a89621d"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Chunk 1: 'Dr. Avinash Kumar Singh\\nHyderabad, India | +91-9005722861 | avinashkumarsingh1986@gmail.com | http://avinashkumarsingh.in\\nProfile\\nWith over 14 years in Al, I have evolved through roles as an ML Resear'\n","Chunk 2: 'n Al, I have evolved through roles as an ML Researcher, Engineer, Product Manager, and now as Chief Al Scientist. | have led the development and deployment of deep learning-based computer vision and N'\n","Chunk 3: 'yment of deep learning-based computer vision and NLP models on platforms like AWS, GCP, Humanoid Robots, Edge Devices like Jetson Nano, Raspberry Pi, and NXP boards. My expertise extends to tackling c'\n","Chunk 4: 'and NXP boards. My expertise extends to tackling challenges in concurrency, security, and latency. My academic journey, enriched by a Ph.D. and postdoctoral research, provides a profound understanding'\n","Chunk 5: 'ctoral research, provides a profound understanding of neural networks across diverse data types, while my industrial experience ensures practical AI solutions are deployed effectively, serving real us'\n","Chunk 6: 'olutions are deployed effectively, serving real users. This unique blend of research and industry expertise enables me to lead in crafting and delivering impactful Al innovations, driving business tra'\n","Chunk 7: 'ing impactful Al innovations, driving business transformation and societal advancement.\\nExperience AI CONSULTANT & CORPORATE TRAINER | ROBAITA, HYDERABAD, INDIA | SEP 24 - CONT...\\n= Mentored and empow'\n","Chunk 8: 'BAD, INDIA | SEP 24 - CONT...\\n= Mentored and empowered 1,500+ students and working professionals from diverse backgrounds. Led immersive sessions on Fine-tuning Large Language Models (LLMs), designing'\n","Chunk 9: 'ine-tuning Large Language Models (LLMs), designing robust Retrieval-Augmented Generation (RAG) systems, building custom AI chatbots, defending LLMs against prompt injection attacks, implementing Model'\n","Chunk 10: 'ainst prompt injection attacks, implementing Model Context Protocol (MCP) for efficient context management, and developing Agentic Al solutions.\\n= Engineered and deployed a cutting-edge visual languag'\n","Chunk 11: 'ineered and deployed a cutting-edge visual language model-based computer vision system to drastically reduce loose picking of Apple iPhones ina warehouse environment, achieving a significant reduction'\n","Chunk 12: 'use environment, achieving a significant reduction from 20% to 5%. This innovative solution leverages visual question answering to enhance product identification, ensuring operational efficiency and a'\n","Chunk 13: 'ntification, ensuring operational efficiency and accuracy.\\n= Designed and developed an advanced conversational AI system tailored to resolve complex challenges within the supply chain. This intelligen'\n","Chunk 14: 'hallenges within the supply chain. This intelligent system facilitates seamless communication with each supply chain node and accurately predicts arrival times by analyzing routes and external factors'\n","Chunk 15: 'val times by analyzing routes and external factors such as weather disruptions, protests, and regulatory conditions, ensuring timely and compliant operations.\\n= Conceptualized and implemented \"Talk to'\n","Chunk 16: 'rations.\\n= Conceptualized and implemented \"Talk to Your Document,\" a generative Al solution that enables interactive question-and-answer sessions with historical documents and databases. This solution'\n","Chunk 17: 'historical documents and databases. This solution achieved impressive BLEU and ROUGE scores of 0.85, demonstrating exceptional accuracy in retrieving and synthesizing relevant information.\\nGLOBAL SOL'\n","Chunk 18: 'and synthesizing relevant information.\\nGLOBAL SOLUTION LEADER | BRANE ENTERPRISES LLP, HYDERABAD, INDIA | MAY 20 - SEP 24\\n= A LLM model is finetuned on financial reports to understand complex financi'\n","Chunk 19: 'on financial reports to understand complex financial reasoning (mostly tables). The system could generate the mathematical formula to process the query, retrieve the argument from the table and comput'\n","Chunk 20: 'y, retrieve the argument from the table and compute the value as the output.\\n= Implemented a Face Recognition-based office attendance system, replacing the existing RFID system, and achieving organiza'\n","Chunk 21: 'g the existing RFID system, and achieving organization-wide deployment.\\n= The system serves 2,856 employees with a 97.63% accuracy rate, resulting in annual savings in operational costs.\\n= Leda ground'\n","Chunk 22: 'annual savings in operational costs.\\n= Leda groundbreaking project to design smart glasses for visually impaired individuals, providing comprehensive assistance in reading, navigation, currency identi'\n","Chunk 23: 'assistance in reading, navigation, currency identification, person recognition, and scene understanding.\\n= The system can detect obstacle up to 5 feet, can help in reading English and six Indian langu'\n","Chunk 24: ', can help in reading English and six Indian languages, could recognize 9,605 objects and labels with 89.76% accuracy.\\n= Successfully delivered a Driver Monitoring System (DMS), incorporating real-tim'\n","Chunk 25: 'er Monitoring System (DMS), incorporating real-time monitoring and safety features. The system tracks driver drowsiness, smoking, drinking, eating, phone usage, and seatbelt compliance, resulting in a'\n","Chunk 26: 'one usage, and seatbelt compliance, resulting in a 40% reduction in driving violations.\\n= Captured and recorded over 1,000 violations with images, date, time, and other details.\\n= Provided live stream'\n","Chunk 27: 'e, time, and other details.\\n= Provided live streams from both interior and exterior dash cameras, enhancing monitoring accuracy.SENIOR RESEARCHER | MONTPELLIER UNIVERSITY, FRANCE | NOV 20- NOV 21\\n= |'\n","Chunk 28: 'NTPELLIER UNIVERSITY, FRANCE | NOV 20- NOV 21\\n= | was associated with the robotics lab (LIRMM) and worked on the EU Project SOPHIA. As the in-charge of Work package 5, I helped my team to coordinate b'\n","Chunk 29: 'f Work package 5, I helped my team to coordinate between different project partners Italian Institute of Technology, Italy, INAIL, Italy, and LIRMM, France for data acquisition, human-robot interactio'\n","Chunk 30: 'rance for data acquisition, human-robot interaction and to derive a deep learning model\\nfor action recognition.\\n= Designed and developed a sensor agnostic, Bidirectional LSTM based deep neural network'\n","Chunk 31: 'stic, Bidirectional LSTM based deep neural network for action recognition. The model is tested in the presence of Xsens suit (used for motion capture) and Intel RealSense and Microsoft Kinect RGB-D da'\n","Chunk 32: 'and Intel RealSense and Microsoft Kinect RGB-D data (3D skeleton). The research is published in 21st International Conference on Humanoid Robots.\\n= The model is integrated with KUKA robot to help hum'\n","Chunk 33: 'he model is integrated with KUKA robot to help human in physical assistance, e.g. carrying object, release object, place object etc. in industrial environment by understanding the human actions.\\nPOST'\n","Chunk 34: 'ironment by understanding the human actions.\\nPOST DOCTORAL RESEARCHER | UMEA UNIVERSITY, SWEDEN | FEB 18 - JAN 20\\n= During this postdoc, | closely worked with Professor Kai-Florian Richter and Profess'\n","Chunk 35: 'ked with Professor Kai-Florian Richter and Professor Thomas HellstrÃ©m. | was a part of intelligent robotics lab, during this postdoc, we designed a dialogue based human robot interaction system that a'\n","Chunk 36: 'alogue based human robot interaction system that allows humans, to talk to the robot. This work was published in a â€˜Aâ€™ rated conference ECAI-2020.\\n= We developed and implemented a robot collaboration'\n","Chunk 37: 'e developed and implemented a robot collaboration framework that enables robots to have dialogues by translating their actions into the natural language. This work was published in Journal of behaviou'\n","Chunk 38: 'ge. This work was published in Journal of behavioural robotics and also featured in Softbank robotics under the best 20 projects in 2020.\\nDEPUTY MANAGER | HCL TECHNOLOGIES, NOIDA, INDIA | FEB 17 - JAN'\n","Chunk 39: 'ER | HCL TECHNOLOGIES, NOIDA, INDIA | FEB 17 - JAN 18\\n= | joined the HCL machine learning division (Noida) when this was a 3 members team. In the span of one year, we conducted 4 successful POCs and g'\n","Chunk 40: 'of one year, we conducted 4 successful POCs and grew the team to a 16 members team.\\n= Asa deputy manager my responsibilities were to handle the client interaction, project scoping, find the place whe'\n","Chunk 41: 't interaction, project scoping, find the place where machine learning solutions can be pitched (to integrated with existing workflows) within and outside the organization.\\n= Helped my team to set up t'\n","Chunk 42: 'ide the organization.\\n= Helped my team to set up the machine learning GPU infrastructure, sketching project roadmaps, resource allocation and tracking. Further motivating team to follow the software e'\n","Chunk 43: '. Further motivating team to follow the software engineering best practices such as maintaining git, hygiene of code (following the coding standards), etc.\\nML ENGINEER | ECLERX SERVICES LIMITED, MUMBA'\n","Chunk 44: 'etc.\\nML ENGINEER | ECLERX SERVICES LIMITED, MUMBAI, INDIA | NOV 15 - FEB 17\\n= eClerx is an Indian IT consulting and outsourcing multinational company. I worked there as a full stack developer and my'\n","Chunk 45: '. I worked there as a full stack developer and my job was to design and develop NLP, ML solutions. We used image pre-processing to improve OCR accuracy, further to integrate these solutions with RPA s'\n","Chunk 46: 'y, further to integrate these solutions with RPA system.\\nEducation PH.D | COMPUTER VISION | DEC 2016] IIIT, ALLAHABAD, INDIA\\nM.TECH | INFORMATION SECURITY | JUNE 2011 | KIIT UNIVERSITY, BHUBANESHWAR,'\n","Chunk 47: 'RITY | JUNE 2011 | KIIT UNIVERSITY, BHUBANESHWAR, INDIA M.SC | INFORMATION TECHNOLOGY | JUNE 2009 | KUMAUN UNIVERSITY, ALMORA, INDIA\\nB.SC | MATHEMATICS | JUNE 2007 | KUMAUN UNIVERSITY, ALMORA, INDIA\\nS'\n","Chunk 48: 'S | JUNE 2007 | KUMAUN UNIVERSITY, ALMORA, INDIA\\nSkills & Abilities\\n= Machine Learning = Machine learning operations (MLOps) = Computer vision = Certified scrum master\\n= Natural language processing ='\n","Chunk 49: 'fied scrum master\\n= Natural language processing = AWS, GCP, E2E cloud platforms\\n= Generative Al, LLM, Vision Language Model = Python, TensorFlow, Pytorch, MySQL\\nPage 2= Genetic Algorithms, Fuzzy Syste'\n","Chunk 50: 'rch, MySQL\\nPage 2= Genetic Algorithms, Fuzzy Systems = Human-Robot Interactions (Nao, Pepper)\\nCertifications\\n= Machine Learning Operations (MLOps) Coursera - Deeplearnining.ai Focus Areas - ML Workflo'\n","Chunk 51: 'rsera - Deeplearnining.ai Focus Areas - ML Workflows, TFX, Model Deployments and Tracking for Production May 3, 2022 = TensorFlow 2 for Deep Learning Coursera - Imperial College London Focus Areas â€” T'\n","Chunk 52: 'Coursera - Imperial College London Focus Areas â€” TensorFlow 2 APIs, Customized Model Training, Probabilistic Deep Learning Jul 9, 2023 = Machine Learning and Soft Computing Indian Statistical Institut'\n","Chunk 53: 'ing and Soft Computing Indian Statistical Institute, Kolkata, India Focus Areas - Artificial Neural Network, Computer Vision and Genetic Algorithms Dec 20, 2012\\nAchievements\\n= SOPHIA - A H2020 EU PROJ'\n","Chunk 54: 'c 20, 2012\\nAchievements\\n= SOPHIA - A H2020 EU PROJECT .\\nSecured three years post-doctoral position on deep learning perception for human-robot collaboration at LIRMM Montpellier.\\n= POST-DOCTORAL FUNDI'\n","Chunk 55: 'ration at LIRMM Montpellier.\\n= POST-DOCTORAL FUNDING FROM KEMPHE .\\nFOUNDATIONS, SWEDEN Received 2 years of research funding to pursue my postdoctoral work in Human-Robot Interaction at Umea University'\n","Chunk 56: 'work in Human-Robot Interaction at Umea University, Sweden.\\n= 1ST POSITION IN M.SC Secured first position in M.Sc for consecutive 2 years at university level.\\nProjects\\nGit, CI/CD, Dockers, Micro Servi'\n","Chunk 57: 'y level.\\nProjects\\nGit, CI/CD, Dockers, Micro Services (REST), JIRA 10T - ESP32, MQTT, Jetson Nano, Raspberry, NXP\\nBuilding Cloud Computing Solutions at Scale Coursera - Duke University\\nFocus Areas - C'\n","Chunk 58: 't Scale Coursera - Duke University\\nFocus Areas - Containers, Cloud APIs, DevOps, AWS an Azure\\nNov 6, 2023\\nScrum Master Certification Specialization Coursera â€” Learn Quest\\nFocus Areas - Agile Methodolo'\n","Chunk 59: 'ursera â€” Learn Quest\\nFocus Areas - Agile Methodology and Product Management\\nFeb 17, 2023\\nPrompt Engineering for ChatGPT\\nVanderbilt University\\nFocus Area - Few Shot Examples, Meta Language Creation Pat'\n","Chunk 60: 'ea - Few Shot Examples, Meta Language Creation Pattern, Different prompts patterns July 9, 2023\\nMARIE SKLODOWSKA CURIE ACTIONS (MSCA) 2020 Applied for the MSCA post-doctoral funding in association wit'\n","Chunk 61: 'the MSCA post-doctoral funding in association with Universitat Rovira i Virgili, Spain and received 85% marks.\\nMHRD, INDIA PH.D FELLOWSHIP\\nReceived 2 years of junior research and 2 years of senior re'\n","Chunk 62: \"years of junior research and 2 years of senior research funding from MHRD for the Ph.D program under robotics and AI lab, IIIT Allahabad.\\nFINETUNENING OF LLM FOR Q&A ON WEBSITE DATA\\nIn today's world,\"\n","Chunk 63: \"G OF LLM FOR Q&A ON WEBSITE DATA\\nIn today's world, simply having an FAQ section or static information on a website isn't enough. We can harness this data to build a conversational Al system that provi\"\n","Chunk 64: 'ata to build a conversational Al system that provides more effective and intuitive search and information retrieval. To tackle this, we fine-tuned the LLaMA model using data scraped from the website.'\n","Chunk 65: 'LLaMA model using data scraped from the website. Initially, we generated question- answer pairs using the website content and ChatGPT. Once we had sufficient data, we used it to fine-tune the model.'\n","Chunk 66: 'fficient data, we used it to fine-tune the model. The model was then deployed as a conversational Al system using OpenWebUI, enabling users to engage with the content in a more interactive manner.\\nTec'\n","Chunk 67: 'with the content in a more interactive manner.\\nTechniques Used: Llama, OpenWebUI, LoRA, Data Preparation, Tokenization and Encoding.\\nPage 3FINANCE LARGE LANGUAGE MODEL\\nWe fine-tuned BERT, an LLM model'\n","Chunk 68: 'GE LANGUAGE MODEL\\nWe fine-tuned BERT, an LLM model, to handle complex reasoning tasks found in financial reports. We utilized the FinQA dataset, which consists of earnings reports from S&P 500 compani'\n","Chunk 69: 'consists of earnings reports from S&P 500 companies spanning from 1999 to 2019. These reports, typically in PDF format, include multiple pages of financial data, often presented in tables and text. T'\n","Chunk 70: 'ancial data, often presented in tables and text. The model generates the mathematical equation necessary to conduct the calculation and further execute that equation on table selected values to get th'\n","Chunk 71: 'e that equation on table selected values to get the output value (execution value). The model achieved an execution accuracy of 65.05%.\\nTechniques Used: BERT, Language Embedding, Program Accuracy, Exe'\n","Chunk 72: 'd: BERT, Language Embedding, Program Accuracy, Execution Accuracy\\nSMART VISION - BRANE ENTERPRISES\\nWe created a smart eyewear prototype equipped with an integrated camera to aid visually impaired indi'\n","Chunk 73: 'an integrated camera to aid visually impaired individuals. Our in-house fabrication of the PCB board enables essential functionalities like Bluetooth connectivity, USB charging, and live-streaming to'\n","Chunk 74: 'connectivity, USB charging, and live-streaming to an Android device. Additionally, we developed a companion app providing features such as object detection, scene analysis, navigation assistance, and'\n","Chunk 75: 'ction, scene analysis, navigation assistance, and person counting. My role involved collaborating with the team to develop and integrate deep learning solutions into the app. Together, we successfully'\n","Chunk 76: 'solutions into the app. Together, we successfully implemented multiple models for object detection, image/dense captioning, currency recognition, OCR, and more.\\nTechniques Used: Faster-RCNN, MobileNe'\n","Chunk 77: ', and more.\\nTechniques Used: Faster-RCNN, MobileNetV2, CNN+LSTM, YOLO\\nINCREMENTAL FACE LEARNING - BRANE ENTERPRISES\\nWe developed an Android application for conducting face recognition using transfer l'\n","Chunk 78: 'n for conducting face recognition using transfer learning techniques. We utilized the FaceNet model as the backbone network and added a classification layer for face classification. One challenge with'\n","Chunk 79: 'layer for face classification. One challenge with neural networks is catastrophic forgetting, where training the entire network (excluding the backbone) on N+1 classes can lead to performance issues.'\n","Chunk 80: 'ne) on N+1 classes can lead to performance issues. To address this, we adopted an incremental learning approach that helps to train and deploy the model in 30 seconds reducing the onboarding time.\\nTec'\n","Chunk 81: 'el in 30 seconds reducing the onboarding time.\\nTechniques Used: Transfer Learning, FaceNet, Incremental Learning\\nACTION RECOGNITION - MONTPELLIER UNIVERSITY\\nGiven one second long measure of the humanâ€™'\n","Chunk 82: 'ERSITY\\nGiven one second long measure of the humanâ€™s motion, the system can determine human action. The originality lies in the use of joint angles, instead of cartesian coordinates. This design choice'\n","Chunk 83: 'stead of cartesian coordinates. This design choice makes the framework sensor agnostic and invariant to affine transformations and to anthropometric differences. On AnDy dataset, we outperform the sta'\n","Chunk 84: 'ifferences. On AnDy dataset, we outperform the state of the art classifier. Furthermore, we show that our system is effective with limited training data, that it is subject independent, and that it is'\n","Chunk 85: 'ta, that it is subject independent, and that it is compatible with robotic real time constraints. In terms of methodology, the system is an original synergy of two antithetical schools of thought: mod'\n","Chunk 86: 'ynergy of two antithetical schools of thought: model based and data-based algorithms. Indeed, it is the cascade of an inverse kinematics estimator compliant with the International Society of Biomechan'\n","Chunk 87: 'pliant with the International Society of Biomechanics recommendations, followed by a deep learning architecture based on Bidirectional Long Short Term Memory. Techniques Used: Bi-LSTM, Mocap, OpenPose'\n","Chunk 88: 'Memory. Techniques Used: Bi-LSTM, Mocap, OpenPose, CNN\\nVISUAL GROUNDING - UMEA UNIVERSITY\\nFor robots to engage with humans in real-world situations or with objects, they must develop a mental represe'\n","Chunk 89: 'r with objects, they must develop a mental representation (\"state of mind\") that a) reflects the robotsâ€™ perception and b) ideally aligns with human comprehension and ideas. Using table-top scenarios'\n","Chunk 90: 'omprehension and ideas. Using table-top scenarios as an example, we propose a framework for generating a robot\\'s \"state of mindâ€ by identifying the objects on the table along with their characteristic'\n","Chunk 91: \"jects on the table along with their characteristics (color, shape, texture) and spatial relationships to one another. The robot's view of the scene is depicted in a dynamic graph where object attribut\"\n","Chunk 92: 'depicted in a dynamic graph where object attributes are translated into fuzzy linguistic variables that correspond to human spatial concepts. This endeavor involves creating these graph representatio'\n","Chunk 93: 'deavor involves creating these graph representations through a combination of low-level neural network-based feature recognition and a high-level fuzzy inference system.\\nTechniques Used: Fuzzy Inferen'\n","Chunk 94: 'y inference system.\\nTechniques Used: Fuzzy Inference System, Mask-RCNN, CNN, Local Binary pattern (LBP), Multi-Layer Perceptron\\nPage 4Talks and Presentations\\n= Title: An empirical review of calibratio'\n","Chunk 95: 'tations\\n= Title: An empirical review of calibration techniques for the Pepper humanoid robotâ€™s RGB and depth camera. Venue: Intelligent systems and application, 5â€˜ Sep 2019, London, England. Occasion:'\n","Chunk 96: 'plication, 5â€˜ Sep 2019, London, England. Occasion: Presented conference paper in IntelliSys 2019. = Fusion of gesture and speech for increased accuracy in human robot interaction. Venue: 25th Internat'\n","Chunk 97: 'y in human robot interaction. Venue: 25th International conference on methods and models in automation and robotics, 24 Aug 2019, Miedzyzdroje, Poland. Occasion: Presented conference paper in MMAR 201'\n","Chunk 98: '. Occasion: Presented conference paper in MMAR 2019. = Conflict Detection and Resolution in Table Top Scenarios for Human-Robot Interaction. Venue: Computing science department, Umea University, 18 Ju'\n","Chunk 99: 'mputing science department, Umea University, 18 Jun, 2019, Umea, Sweden. Occasion: Poster presentation in 31st Swedish Al Society Workshop. = Deep learning and its applications. Venue: UFBI department'\n","Chunk 100: 'rning and its applications. Venue: UFBI department, Umea University, 15\" Jun 2018, Umea, Sweden. Occasion: Invited as a speaker at Umea center for Functional Brain Imaging (UFBI) day. = Sketch drawing'\n","Chunk 101: 'ctional Brain Imaging (UFBI) day. = Sketch drawing by NAO humanoid robot. Venue: TENCON a premier international technical conference of IEEE Region 10, 1st Nov, 2015, Macau, China. Occasion: Presented'\n","Chunk 102: ', 1st Nov, 2015, Macau, China. Occasion: Presented conference paper in TENCON 2015.\\nSelected Publications [ Journals and Conferences]\\nJOURNAL PUBLICATIONS\\n= Singh, A. K., Baranwal, N., Richter, K. F.,'\n","Chunk 103: 'IONS\\n= Singh, A. K., Baranwal, N., Richter, K. F., Hellstrom, T., & Bensch, S. (2020). Verbal explanations by collaborating robot teams. Paladyn, Journal of Behavioral Robotics, 12(1), 47-57.\\n= Singh,'\n","Chunk 104: 'nal of Behavioral Robotics, 12(1), 47-57.\\n= Singh, A. K., Baranwal, N., & Nandi, G. C. (2019). A rough set based reasoning approach for criminal identification. International Journal of Machine Learni'\n","Chunk 105: 'ification. International Journal of Machine Learning and Cybernetics, 10, 413-431.\\n= Baranwal, N., Nandi, G.C., & Singh, A. K. (2017). Real-Time Gesture-Based Communication Using Possibility Theory- B'\n","Chunk 106: 're-Based Communication Using Possibility Theory- Based Hidden Markov Model. Computational Intelligence, 33(4), 843-862.\\n= Baranwal, N., Singh, A. K., & Nandi, G. C. (2017). Development of a framework'\n","Chunk 107: '& Nandi, G. C. (2017). Development of a framework for human-robot interactions with Indian sign language using possibility theory. International Journal of Social Robotics, 9, 563-574.\\n= Singh, A. K.,'\n","Chunk 108: 'al of Social Robotics, 9, 563-574.\\n= Singh, A. K., Baranwal, N., & Nandi, G. C. (2017). Development of a self reliant humanoid robot for sketch drawing. Multimedia Tools and Applications, 76, 18847-18'\n","Chunk 109: 'g. Multimedia Tools and Applications, 76, 18847-18870.\\nCONFERENCE PUBLICATIONS\\n= Singh, A. K.,, Adjel, M., Bonnet, V., Passama, R., & Cherubini, A. (2022, November). A framework for recognizing indust'\n","Chunk 110: '022, November). A framework for recognizing industrial actions via joint angles. In 2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids) (pp. 210-216). IEEE.\\n= Kumar Singh, A., B'\n","Chunk 111: 'manoids) (pp. 210-216). IEEE.\\n= Kumar Singh, A., Baranwal, N., & Richter, K. F. (2020). A fuzzy inference system for a visually grounded robot state of mind. In ECAI 2020 (pp. 2402-2409). IOS Press.\\n='\n","Chunk 112: 'f mind. In ECAI 2020 (pp. 2402-2409). IOS Press.\\n= Singh, A. K., Baranwal, N., & Richter, K. F. (2020). An empirical review of calibration techniques for the pepper humanoid robotâ€™s RGB and depth came'\n","Chunk 113: 'for the pepper humanoid robotâ€™s RGB and depth camera. In Intelligent Systems and Applications: Proceedings of the 2019 Intelligent Systems Conference (IntelliSys) Volume 2 (pp. 1026-1038). Springer In'\n","Chunk 114: '(IntelliSys) Volume 2 (pp. 1026-1038). Springer International Publishing.\\n= Singh, A. K., Chakraborty, P., & Nandi, G. C. (2015, November). Sketch drawing by nao humanoid robot. In TENCON 2015-2015 IE'\n","Chunk 115: 'wing by nao humanoid robot. In TENCON 2015-2015 IEEE Region 10 Conference (pp. 1-6). IEEE.\\nPlease find the complete list of publication at my Google Scholar Profile.\\nOnline Presence\\nGithub: https://gi'\n","Chunk 116: 'cholar Profile.\\nOnline Presence\\nGithub: https://github.com/robaita LinkedIn: https://fr.linkedin.com/in/dr-avinash-kumar-singh-2a570a31\\nPage 5'\n"]}]},{"cell_type":"markdown","source":["### ğŸ”¹ Semantic Chunking\n","\n","**Definition:** Splits text using natural language boundaries (sentences/paragraphs).\n","\n","**How it Works:** Splits by paragraphs, then sentences, then characters if necessary.\n"],"metadata":{"id":"eXYaZ0bT66am"}},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","semantic_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=200,\n","    chunk_overlap=50,\n","    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",")\n","semantic_chunks = semantic_splitter.split_text(text)\n","for i, chunk in enumerate(semantic_chunks):\n","    print(f\"Chunk {i+1}: {repr(chunk)}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BgPv-DZ67Ev","executionInfo":{"status":"ok","timestamp":1748454521371,"user_tz":-330,"elapsed":57,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"1d74be60-5401-4b5c-cf33-f2e523c6ed51"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Chunk 1: 'Dr. Avinash Kumar Singh\\nHyderabad, India | +91-9005722861 | avinashkumarsingh1986@gmail.com | http://avinashkumarsingh.in\\nProfile'\n","Chunk 2: 'With over 14 years in Al, I have evolved through roles as an ML Researcher, Engineer, Product Manager, and now as Chief Al Scientist'\n","Chunk 3: '. | have led the development and deployment of deep learning-based computer vision and NLP models on platforms like AWS, GCP, Humanoid Robots, Edge Devices like Jetson Nano, Raspberry Pi, and NXP'\n","Chunk 4: 'Devices like Jetson Nano, Raspberry Pi, and NXP boards'\n","Chunk 5: '. My expertise extends to tackling challenges in concurrency, security, and latency. My academic journey, enriched by a Ph.D'\n","Chunk 6: '. and postdoctoral research, provides a profound understanding of neural networks across diverse data types, while my industrial experience ensures practical AI solutions are deployed effectively,'\n","Chunk 7: 'practical AI solutions are deployed effectively, serving real users'\n","Chunk 8: '. This unique blend of research and industry expertise enables me to lead in crafting and delivering impactful Al innovations, driving business transformation and societal advancement.'\n","Chunk 9: 'Experience AI CONSULTANT & CORPORATE TRAINER | ROBAITA, HYDERABAD, INDIA | SEP 24 - CONT...'\n","Chunk 10: '= Mentored and empowered 1,500+ students and working professionals from diverse backgrounds'\n","Chunk 11: '. Led immersive sessions on Fine-tuning Large Language Models (LLMs), designing robust Retrieval-Augmented Generation (RAG) systems, building custom AI chatbots, defending LLMs against prompt'\n","Chunk 12: 'custom AI chatbots, defending LLMs against prompt injection attacks, implementing Model Context Protocol (MCP) for efficient context management, and developing Agentic Al solutions'\n","Chunk 13: '.'\n","Chunk 14: '= Engineered and deployed a cutting-edge visual language model-based computer vision system to drastically reduce loose picking of Apple iPhones ina warehouse environment, achieving a significant'\n","Chunk 15: 'warehouse environment, achieving a significant reduction from 20% to 5%'\n","Chunk 16: '. This innovative solution leverages visual question answering to enhance product identification, ensuring operational efficiency and accuracy.'\n","Chunk 17: '= Designed and developed an advanced conversational AI system tailored to resolve complex challenges within the supply chain'\n","Chunk 18: '. This intelligent system facilitates seamless communication with each supply chain node and accurately predicts arrival times by analyzing routes and external factors such as weather disruptions,'\n","Chunk 19: 'and external factors such as weather disruptions, protests, and regulatory conditions, ensuring timely and compliant operations'\n","Chunk 20: '.'\n","Chunk 21: '= Conceptualized and implemented \"Talk to Your Document,\" a generative Al solution that enables interactive question-and-answer sessions with historical documents and databases'\n","Chunk 22: '. This solution achieved impressive BLEU and ROUGE scores of 0.85, demonstrating exceptional accuracy in retrieving and synthesizing relevant information.'\n","Chunk 23: 'GLOBAL SOLUTION LEADER | BRANE ENTERPRISES LLP, HYDERABAD, INDIA | MAY 20 - SEP 24'\n","Chunk 24: '= A LLM model is finetuned on financial reports to understand complex financial reasoning (mostly tables)'\n","Chunk 25: '. The system could generate the mathematical formula to process the query, retrieve the argument from the table and compute the value as the output.'\n","Chunk 26: '= Implemented a Face Recognition-based office attendance system, replacing the existing RFID system, and achieving organization-wide deployment.'\n","Chunk 27: '= The system serves 2,856 employees with a 97.63% accuracy rate, resulting in annual savings in operational costs.'\n","Chunk 28: '= Leda groundbreaking project to design smart glasses for visually impaired individuals, providing comprehensive assistance in reading, navigation, currency identification, person recognition, and'\n","Chunk 29: 'currency identification, person recognition, and scene understanding'\n","Chunk 30: '.'\n","Chunk 31: '= The system can detect obstacle up to 5 feet, can help in reading English and six Indian languages, could recognize 9,605 objects and labels with 89.76% accuracy.'\n","Chunk 32: '= Successfully delivered a Driver Monitoring System (DMS), incorporating real-time monitoring and safety features'\n","Chunk 33: '. The system tracks driver drowsiness, smoking, drinking, eating, phone usage, and seatbelt compliance, resulting in a 40% reduction in driving violations.'\n","Chunk 34: '= Captured and recorded over 1,000 violations with images, date, time, and other details.'\n","Chunk 35: '= Provided live streams from both interior and exterior dash cameras, enhancing monitoring accuracy.SENIOR RESEARCHER | MONTPELLIER UNIVERSITY, FRANCE | NOV 20- NOV 21'\n","Chunk 36: '= | was associated with the robotics lab (LIRMM) and worked on the EU Project SOPHIA'\n","Chunk 37: '. As the in-charge of Work package 5, I helped my team to coordinate between different project partners Italian Institute of Technology, Italy, INAIL, Italy, and LIRMM, France for data acquisition,'\n","Chunk 38: 'Italy, and LIRMM, France for data acquisition, human-robot interaction and to derive a deep learning model'\n","Chunk 39: 'for action recognition.'\n","Chunk 40: '= Designed and developed a sensor agnostic, Bidirectional LSTM based deep neural network for action recognition'\n","Chunk 41: '. The model is tested in the presence of Xsens suit (used for motion capture) and Intel RealSense and Microsoft Kinect RGB-D data (3D skeleton)'\n","Chunk 42: '. The research is published in 21st International Conference on Humanoid Robots.'\n","Chunk 43: '= The model is integrated with KUKA robot to help human in physical assistance, e.g. carrying object, release object, place object etc. in industrial environment by understanding the human actions.'\n","Chunk 44: 'POST DOCTORAL RESEARCHER | UMEA UNIVERSITY, SWEDEN | FEB 18 - JAN 20'\n","Chunk 45: '= During this postdoc, | closely worked with Professor Kai-Florian Richter and Professor Thomas HellstrÃ©m'\n","Chunk 46: '. | was a part of intelligent robotics lab, during this postdoc, we designed a dialogue based human robot interaction system that allows humans, to talk to the robot'\n","Chunk 47: '. This work was published in a â€˜Aâ€™ rated conference ECAI-2020.'\n","Chunk 48: '= We developed and implemented a robot collaboration framework that enables robots to have dialogues by translating their actions into the natural language'\n","Chunk 49: '. This work was published in Journal of behavioural robotics and also featured in Softbank robotics under the best 20 projects in 2020.'\n","Chunk 50: 'DEPUTY MANAGER | HCL TECHNOLOGIES, NOIDA, INDIA | FEB 17 - JAN 18'\n","Chunk 51: '= | joined the HCL machine learning division (Noida) when this was a 3 members team. In the span of one year, we conducted 4 successful POCs and grew the team to a 16 members team.'\n","Chunk 52: '= Asa deputy manager my responsibilities were to handle the client interaction, project scoping, find the place where machine learning solutions can be pitched (to integrated with existing workflows)'\n","Chunk 53: 'pitched (to integrated with existing workflows) within and outside the organization'\n","Chunk 54: '.'\n","Chunk 55: '= Helped my team to set up the machine learning GPU infrastructure, sketching project roadmaps, resource allocation and tracking'\n","Chunk 56: '. Further motivating team to follow the software engineering best practices such as maintaining git, hygiene of code (following the coding standards), etc.'\n","Chunk 57: 'ML ENGINEER | ECLERX SERVICES LIMITED, MUMBAI, INDIA | NOV 15 - FEB 17'\n","Chunk 58: '= eClerx is an Indian IT consulting and outsourcing multinational company. I worked there as a full stack developer and my job was to design and develop NLP, ML solutions'\n","Chunk 59: '. We used image pre-processing to improve OCR accuracy, further to integrate these solutions with RPA system.'\n","Chunk 60: 'Education PH.D | COMPUTER VISION | DEC 2016] IIIT, ALLAHABAD, INDIA'\n","Chunk 61: 'M.TECH | INFORMATION SECURITY | JUNE 2011 | KIIT UNIVERSITY, BHUBANESHWAR, INDIA M.SC | INFORMATION TECHNOLOGY | JUNE 2009 | KUMAUN UNIVERSITY, ALMORA, INDIA'\n","Chunk 62: 'B.SC | MATHEMATICS | JUNE 2007 | KUMAUN UNIVERSITY, ALMORA, INDIA\\nSkills & Abilities\\n= Machine Learning = Machine learning operations (MLOps) = Computer vision = Certified scrum master'\n","Chunk 63: '= Natural language processing = AWS, GCP, E2E cloud platforms\\n= Generative Al, LLM, Vision Language Model = Python, TensorFlow, Pytorch, MySQL'\n","Chunk 64: 'Page 2= Genetic Algorithms, Fuzzy Systems = Human-Robot Interactions (Nao, Pepper)\\nCertifications'\n","Chunk 65: '= Machine Learning Operations (MLOps) Coursera - Deeplearnining'\n","Chunk 66: '.ai Focus Areas - ML Workflows, TFX, Model Deployments and Tracking for Production May 3, 2022 = TensorFlow 2 for Deep Learning Coursera - Imperial College London Focus Areas â€” TensorFlow 2 APIs,'\n","Chunk 67: 'College London Focus Areas â€” TensorFlow 2 APIs, Customized Model Training, Probabilistic Deep Learning Jul 9, 2023 = Machine Learning and Soft Computing Indian Statistical Institute, Kolkata, India'\n","Chunk 68: 'Indian Statistical Institute, Kolkata, India Focus Areas - Artificial Neural Network, Computer Vision and Genetic Algorithms Dec 20, 2012'\n","Chunk 69: 'Achievements\\n= SOPHIA - A H2020 EU PROJECT .\\nSecured three years post-doctoral position on deep learning perception for human-robot collaboration at LIRMM Montpellier.'\n","Chunk 70: '= POST-DOCTORAL FUNDING FROM KEMPHE .\\nFOUNDATIONS, SWEDEN Received 2 years of research funding to pursue my postdoctoral work in Human-Robot Interaction at Umea University, Sweden.'\n","Chunk 71: '= 1ST POSITION IN M.SC Secured first position in M.Sc for consecutive 2 years at university level.\\nProjects'\n","Chunk 72: 'Projects\\nGit, CI/CD, Dockers, Micro Services (REST), JIRA 10T - ESP32, MQTT, Jetson Nano, Raspberry, NXP\\nBuilding Cloud Computing Solutions at Scale Coursera - Duke University'\n","Chunk 73: 'Focus Areas - Containers, Cloud APIs, DevOps, AWS an Azure\\nNov 6, 2023\\nScrum Master Certification Specialization Coursera â€” Learn Quest\\nFocus Areas - Agile Methodology and Product Management'\n","Chunk 74: 'Feb 17, 2023\\nPrompt Engineering for ChatGPT\\nVanderbilt University\\nFocus Area - Few Shot Examples, Meta Language Creation Pattern, Different prompts patterns July 9, 2023'\n","Chunk 75: 'MARIE SKLODOWSKA CURIE ACTIONS (MSCA) 2020 Applied for the MSCA post-doctoral funding in association with Universitat Rovira i Virgili, Spain and received 85% marks.\\nMHRD, INDIA PH.D FELLOWSHIP'\n","Chunk 76: 'MHRD, INDIA PH.D FELLOWSHIP\\nReceived 2 years of junior research and 2 years of senior research funding from MHRD for the Ph.D program under robotics and AI lab, IIIT Allahabad.'\n","Chunk 77: 'FINETUNENING OF LLM FOR Q&A ON WEBSITE DATA'\n","Chunk 78: \"In today's world, simply having an FAQ section or static information on a website isn't enough\"\n","Chunk 79: '. We can harness this data to build a conversational Al system that provides more effective and intuitive search and information retrieval'\n","Chunk 80: '. To tackle this, we fine-tuned the LLaMA model using data scraped from the website. Initially, we generated question- answer pairs using the website content and ChatGPT'\n","Chunk 81: '. Once we had sufficient data, we used it to fine-tune the model'\n","Chunk 82: '. The model was then deployed as a conversational Al system using OpenWebUI, enabling users to engage with the content in a more interactive manner.'\n","Chunk 83: 'Techniques Used: Llama, OpenWebUI, LoRA, Data Preparation, Tokenization and Encoding.\\nPage 3FINANCE LARGE LANGUAGE MODEL'\n","Chunk 84: 'We fine-tuned BERT, an LLM model, to handle complex reasoning tasks found in financial reports'\n","Chunk 85: '. We utilized the FinQA dataset, which consists of earnings reports from S&P 500 companies spanning from 1999 to 2019'\n","Chunk 86: '. These reports, typically in PDF format, include multiple pages of financial data, often presented in tables and text'\n","Chunk 87: '. The model generates the mathematical equation necessary to conduct the calculation and further execute that equation on table selected values to get the output value (execution value)'\n","Chunk 88: '. The model achieved an execution accuracy of 65.05%.'\n","Chunk 89: 'Techniques Used: BERT, Language Embedding, Program Accuracy, Execution Accuracy\\nSMART VISION - BRANE ENTERPRISES'\n","Chunk 90: 'We created a smart eyewear prototype equipped with an integrated camera to aid visually impaired individuals'\n","Chunk 91: '. Our in-house fabrication of the PCB board enables essential functionalities like Bluetooth connectivity, USB charging, and live-streaming to an Android device'\n","Chunk 92: '. Additionally, we developed a companion app providing features such as object detection, scene analysis, navigation assistance, and person counting'\n","Chunk 93: '. My role involved collaborating with the team to develop and integrate deep learning solutions into the app'\n","Chunk 94: '. Together, we successfully implemented multiple models for object detection, image/dense captioning, currency recognition, OCR, and more.'\n","Chunk 95: 'Techniques Used: Faster-RCNN, MobileNetV2, CNN+LSTM, YOLO\\nINCREMENTAL FACE LEARNING - BRANE ENTERPRISES'\n","Chunk 96: 'We developed an Android application for conducting face recognition using transfer learning techniques'\n","Chunk 97: '. We utilized the FaceNet model as the backbone network and added a classification layer for face classification'\n","Chunk 98: '. One challenge with neural networks is catastrophic forgetting, where training the entire network (excluding the backbone) on N+1 classes can lead to performance issues'\n","Chunk 99: '. To address this, we adopted an incremental learning approach that helps to train and deploy the model in 30 seconds reducing the onboarding time.'\n","Chunk 100: 'Techniques Used: Transfer Learning, FaceNet, Incremental Learning\\nACTION RECOGNITION - MONTPELLIER UNIVERSITY'\n","Chunk 101: 'Given one second long measure of the humanâ€™s motion, the system can determine human action. The originality lies in the use of joint angles, instead of cartesian coordinates'\n","Chunk 102: '. This design choice makes the framework sensor agnostic and invariant to affine transformations and to anthropometric differences. On AnDy dataset, we outperform the state of the art classifier'\n","Chunk 103: '. Furthermore, we show that our system is effective with limited training data, that it is subject independent, and that it is compatible with robotic real time constraints'\n","Chunk 104: '. In terms of methodology, the system is an original synergy of two antithetical schools of thought: model based and data-based algorithms'\n","Chunk 105: '. Indeed, it is the cascade of an inverse kinematics estimator compliant with the International Society of Biomechanics recommendations, followed by a deep learning architecture based on Bidirectional'\n","Chunk 106: 'deep learning architecture based on Bidirectional Long Short Term Memory'\n","Chunk 107: '. Techniques Used: Bi-LSTM, Mocap, OpenPose, CNN'\n","Chunk 108: 'VISUAL GROUNDING - UMEA UNIVERSITY'\n","Chunk 109: 'For robots to engage with humans in real-world situations or with objects, they must develop a mental representation (\"state of mind\") that a) reflects the robotsâ€™ perception and b) ideally aligns'\n","Chunk 110: 'the robotsâ€™ perception and b) ideally aligns with human comprehension and ideas'\n","Chunk 111: '. Using table-top scenarios as an example, we propose a framework for generating a robot\\'s \"state of mindâ€ by identifying the objects on the table along with their characteristics (color, shape,'\n","Chunk 112: 'along with their characteristics (color, shape, texture) and spatial relationships to one another'\n","Chunk 113: \". The robot's view of the scene is depicted in a dynamic graph where object attributes are translated into fuzzy linguistic variables that correspond to human spatial concepts\"\n","Chunk 114: '. This endeavor involves creating these graph representations through a combination of low-level neural network-based feature recognition and a high-level fuzzy inference system.'\n","Chunk 115: 'Techniques Used: Fuzzy Inference System, Mask-RCNN, CNN, Local Binary pattern (LBP), Multi-Layer Perceptron\\nPage 4Talks and Presentations'\n","Chunk 116: '= Title: An empirical review of calibration techniques for the Pepper humanoid robotâ€™s RGB and depth camera. Venue: Intelligent systems and application, 5â€˜ Sep 2019, London, England'\n","Chunk 117: '. Occasion: Presented conference paper in IntelliSys 2019. = Fusion of gesture and speech for increased accuracy in human robot interaction'\n","Chunk 118: '. Venue: 25th International conference on methods and models in automation and robotics, 24 Aug 2019, Miedzyzdroje, Poland. Occasion: Presented conference paper in MMAR 2019'\n","Chunk 119: '. = Conflict Detection and Resolution in Table Top Scenarios for Human-Robot Interaction. Venue: Computing science department, Umea University, 18 Jun, 2019, Umea, Sweden'\n","Chunk 120: '. Occasion: Poster presentation in 31st Swedish Al Society Workshop. = Deep learning and its applications. Venue: UFBI department, Umea University, 15\" Jun 2018, Umea, Sweden'\n","Chunk 121: '. Occasion: Invited as a speaker at Umea center for Functional Brain Imaging (UFBI) day. = Sketch drawing by NAO humanoid robot'\n","Chunk 122: '. = Sketch drawing by NAO humanoid robot. Venue: TENCON a premier international technical conference of IEEE Region 10, 1st Nov, 2015, Macau, China. Occasion: Presented conference paper in TENCON 2015'\n","Chunk 123: '.'\n","Chunk 124: 'Selected Publications [ Journals and Conferences]\\nJOURNAL PUBLICATIONS'\n","Chunk 125: '= Singh, A. K., Baranwal, N., Richter, K. F., Hellstrom, T., & Bensch, S. (2020). Verbal explanations by collaborating robot teams. Paladyn, Journal of Behavioral Robotics, 12(1), 47-57.'\n","Chunk 126: '= Singh, A. K., Baranwal, N., & Nandi, G. C. (2019). A rough set based reasoning approach for criminal identification. International Journal of Machine Learning and Cybernetics, 10, 413-431.'\n","Chunk 127: '= Baranwal, N., Nandi, G.C., & Singh, A. K. (2017). Real-Time Gesture-Based Communication Using Possibility Theory- Based Hidden Markov Model. Computational Intelligence, 33(4), 843-862.'\n","Chunk 128: '= Baranwal, N., Singh, A. K., & Nandi, G. C. (2017). Development of a framework for human-robot interactions with Indian sign language using possibility theory'\n","Chunk 129: '. International Journal of Social Robotics, 9, 563-574.'\n","Chunk 130: '= Singh, A. K., Baranwal, N., & Nandi, G. C. (2017). Development of a self reliant humanoid robot for sketch drawing. Multimedia Tools and Applications, 76, 18847-18870.\\nCONFERENCE PUBLICATIONS'\n","Chunk 131: '= Singh, A. K.,, Adjel, M., Bonnet, V., Passama, R., & Cherubini, A. (2022, November). A framework for recognizing industrial actions via joint angles'\n","Chunk 132: '. In 2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids) (pp. 210-216). IEEE.'\n","Chunk 133: '= Kumar Singh, A., Baranwal, N., & Richter, K. F. (2020). A fuzzy inference system for a visually grounded robot state of mind. In ECAI 2020 (pp. 2402-2409). IOS Press.'\n","Chunk 134: '= Singh, A. K., Baranwal, N., & Richter, K. F. (2020). An empirical review of calibration techniques for the pepper humanoid robotâ€™s RGB and depth camera'\n","Chunk 135: '. In Intelligent Systems and Applications: Proceedings of the 2019 Intelligent Systems Conference (IntelliSys) Volume 2 (pp. 1026-1038). Springer International Publishing.'\n","Chunk 136: '= Singh, A. K., Chakraborty, P., & Nandi, G. C. (2015, November). Sketch drawing by nao humanoid robot. In TENCON 2015-2015 IEEE Region 10 Conference (pp. 1-6). IEEE.'\n","Chunk 137: 'Please find the complete list of publication at my Google Scholar Profile.\\nOnline Presence\\nGithub: https://github.com/robaita LinkedIn: https://fr.linkedin.com/in/dr-avinash-kumar-singh-2a570a31'\n","Chunk 138: 'Page 5'\n"]}]},{"cell_type":"markdown","source":["### ğŸ”¹ Sliding Window Chunking\n","**Definition:** Creates overlapping chunks to preserve continuity.\n","**How it Works:** Moves the window forward with some overlap to maintain flow."],"metadata":{"id":"3xQNteY-7Ht8"}},{"cell_type":"code","source":["window_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50, separator = \"\")\n","window_chunks = window_splitter.split_text(text)\n","for i, chunk in enumerate(window_chunks):\n","    print(f\"Chunk {i+1}: {repr(chunk)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0msZ9WjH7QzY","executionInfo":{"status":"ok","timestamp":1748454773978,"user_tz":-330,"elapsed":95,"user":{"displayName":"Dr. Avinash Kumar Singh","userId":"03688237427667000371"}},"outputId":"0d3cc6a6-b6af-4dfa-8334-29769911e4ec"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Chunk 1: 'Dr. Avinash Kumar Singh\\nHyderabad, India | +91-9005722861 | avinashkumarsingh1986@gmail.com | http://avinashkumarsingh.in\\nProfile\\nWith over 14 years in Al, I have evolved through roles as an ML Resear'\n","Chunk 2: 'n Al, I have evolved through roles as an ML Researcher, Engineer, Product Manager, and now as Chief Al Scientist. | have led the development and deployment of deep learning-based computer vision and N'\n","Chunk 3: 'yment of deep learning-based computer vision and NLP models on platforms like AWS, GCP, Humanoid Robots, Edge Devices like Jetson Nano, Raspberry Pi, and NXP boards. My expertise extends to tackling c'\n","Chunk 4: 'and NXP boards. My expertise extends to tackling challenges in concurrency, security, and latency. My academic journey, enriched by a Ph.D. and postdoctoral research, provides a profound understanding'\n","Chunk 5: 'ctoral research, provides a profound understanding of neural networks across diverse data types, while my industrial experience ensures practical AI solutions are deployed effectively, serving real us'\n","Chunk 6: 'olutions are deployed effectively, serving real users. This unique blend of research and industry expertise enables me to lead in crafting and delivering impactful Al innovations, driving business tra'\n","Chunk 7: 'ing impactful Al innovations, driving business transformation and societal advancement.\\nExperience AI CONSULTANT & CORPORATE TRAINER | ROBAITA, HYDERABAD, INDIA | SEP 24 - CONT...\\n= Mentored and empow'\n","Chunk 8: 'BAD, INDIA | SEP 24 - CONT...\\n= Mentored and empowered 1,500+ students and working professionals from diverse backgrounds. Led immersive sessions on Fine-tuning Large Language Models (LLMs), designing'\n","Chunk 9: 'ine-tuning Large Language Models (LLMs), designing robust Retrieval-Augmented Generation (RAG) systems, building custom AI chatbots, defending LLMs against prompt injection attacks, implementing Model'\n","Chunk 10: 'ainst prompt injection attacks, implementing Model Context Protocol (MCP) for efficient context management, and developing Agentic Al solutions.\\n= Engineered and deployed a cutting-edge visual languag'\n","Chunk 11: 'ineered and deployed a cutting-edge visual language model-based computer vision system to drastically reduce loose picking of Apple iPhones ina warehouse environment, achieving a significant reduction'\n","Chunk 12: 'use environment, achieving a significant reduction from 20% to 5%. This innovative solution leverages visual question answering to enhance product identification, ensuring operational efficiency and a'\n","Chunk 13: 'ntification, ensuring operational efficiency and accuracy.\\n= Designed and developed an advanced conversational AI system tailored to resolve complex challenges within the supply chain. This intelligen'\n","Chunk 14: 'hallenges within the supply chain. This intelligent system facilitates seamless communication with each supply chain node and accurately predicts arrival times by analyzing routes and external factors'\n","Chunk 15: 'val times by analyzing routes and external factors such as weather disruptions, protests, and regulatory conditions, ensuring timely and compliant operations.\\n= Conceptualized and implemented \"Talk to'\n","Chunk 16: 'rations.\\n= Conceptualized and implemented \"Talk to Your Document,\" a generative Al solution that enables interactive question-and-answer sessions with historical documents and databases. This solution'\n","Chunk 17: 'historical documents and databases. This solution achieved impressive BLEU and ROUGE scores of 0.85, demonstrating exceptional accuracy in retrieving and synthesizing relevant information.\\nGLOBAL SOL'\n","Chunk 18: 'and synthesizing relevant information.\\nGLOBAL SOLUTION LEADER | BRANE ENTERPRISES LLP, HYDERABAD, INDIA | MAY 20 - SEP 24\\n= A LLM model is finetuned on financial reports to understand complex financi'\n","Chunk 19: 'on financial reports to understand complex financial reasoning (mostly tables). The system could generate the mathematical formula to process the query, retrieve the argument from the table and comput'\n","Chunk 20: 'y, retrieve the argument from the table and compute the value as the output.\\n= Implemented a Face Recognition-based office attendance system, replacing the existing RFID system, and achieving organiza'\n","Chunk 21: 'g the existing RFID system, and achieving organization-wide deployment.\\n= The system serves 2,856 employees with a 97.63% accuracy rate, resulting in annual savings in operational costs.\\n= Leda ground'\n","Chunk 22: 'annual savings in operational costs.\\n= Leda groundbreaking project to design smart glasses for visually impaired individuals, providing comprehensive assistance in reading, navigation, currency identi'\n","Chunk 23: 'assistance in reading, navigation, currency identification, person recognition, and scene understanding.\\n= The system can detect obstacle up to 5 feet, can help in reading English and six Indian langu'\n","Chunk 24: ', can help in reading English and six Indian languages, could recognize 9,605 objects and labels with 89.76% accuracy.\\n= Successfully delivered a Driver Monitoring System (DMS), incorporating real-tim'\n","Chunk 25: 'er Monitoring System (DMS), incorporating real-time monitoring and safety features. The system tracks driver drowsiness, smoking, drinking, eating, phone usage, and seatbelt compliance, resulting in a'\n","Chunk 26: 'one usage, and seatbelt compliance, resulting in a 40% reduction in driving violations.\\n= Captured and recorded over 1,000 violations with images, date, time, and other details.\\n= Provided live stream'\n","Chunk 27: 'e, time, and other details.\\n= Provided live streams from both interior and exterior dash cameras, enhancing monitoring accuracy.SENIOR RESEARCHER | MONTPELLIER UNIVERSITY, FRANCE | NOV 20- NOV 21\\n= |'\n","Chunk 28: 'NTPELLIER UNIVERSITY, FRANCE | NOV 20- NOV 21\\n= | was associated with the robotics lab (LIRMM) and worked on the EU Project SOPHIA. As the in-charge of Work package 5, I helped my team to coordinate b'\n","Chunk 29: 'f Work package 5, I helped my team to coordinate between different project partners Italian Institute of Technology, Italy, INAIL, Italy, and LIRMM, France for data acquisition, human-robot interactio'\n","Chunk 30: 'rance for data acquisition, human-robot interaction and to derive a deep learning model\\nfor action recognition.\\n= Designed and developed a sensor agnostic, Bidirectional LSTM based deep neural network'\n","Chunk 31: 'stic, Bidirectional LSTM based deep neural network for action recognition. The model is tested in the presence of Xsens suit (used for motion capture) and Intel RealSense and Microsoft Kinect RGB-D da'\n","Chunk 32: 'and Intel RealSense and Microsoft Kinect RGB-D data (3D skeleton). The research is published in 21st International Conference on Humanoid Robots.\\n= The model is integrated with KUKA robot to help hum'\n","Chunk 33: 'he model is integrated with KUKA robot to help human in physical assistance, e.g. carrying object, release object, place object etc. in industrial environment by understanding the human actions.\\nPOST'\n","Chunk 34: 'ironment by understanding the human actions.\\nPOST DOCTORAL RESEARCHER | UMEA UNIVERSITY, SWEDEN | FEB 18 - JAN 20\\n= During this postdoc, | closely worked with Professor Kai-Florian Richter and Profess'\n","Chunk 35: 'ked with Professor Kai-Florian Richter and Professor Thomas HellstrÃ©m. | was a part of intelligent robotics lab, during this postdoc, we designed a dialogue based human robot interaction system that a'\n","Chunk 36: 'alogue based human robot interaction system that allows humans, to talk to the robot. This work was published in a â€˜Aâ€™ rated conference ECAI-2020.\\n= We developed and implemented a robot collaboration'\n","Chunk 37: 'e developed and implemented a robot collaboration framework that enables robots to have dialogues by translating their actions into the natural language. This work was published in Journal of behaviou'\n","Chunk 38: 'ge. This work was published in Journal of behavioural robotics and also featured in Softbank robotics under the best 20 projects in 2020.\\nDEPUTY MANAGER | HCL TECHNOLOGIES, NOIDA, INDIA | FEB 17 - JAN'\n","Chunk 39: 'ER | HCL TECHNOLOGIES, NOIDA, INDIA | FEB 17 - JAN 18\\n= | joined the HCL machine learning division (Noida) when this was a 3 members team. In the span of one year, we conducted 4 successful POCs and g'\n","Chunk 40: 'of one year, we conducted 4 successful POCs and grew the team to a 16 members team.\\n= Asa deputy manager my responsibilities were to handle the client interaction, project scoping, find the place whe'\n","Chunk 41: 't interaction, project scoping, find the place where machine learning solutions can be pitched (to integrated with existing workflows) within and outside the organization.\\n= Helped my team to set up t'\n","Chunk 42: 'ide the organization.\\n= Helped my team to set up the machine learning GPU infrastructure, sketching project roadmaps, resource allocation and tracking. Further motivating team to follow the software e'\n","Chunk 43: '. Further motivating team to follow the software engineering best practices such as maintaining git, hygiene of code (following the coding standards), etc.\\nML ENGINEER | ECLERX SERVICES LIMITED, MUMBA'\n","Chunk 44: 'etc.\\nML ENGINEER | ECLERX SERVICES LIMITED, MUMBAI, INDIA | NOV 15 - FEB 17\\n= eClerx is an Indian IT consulting and outsourcing multinational company. I worked there as a full stack developer and my'\n","Chunk 45: '. I worked there as a full stack developer and my job was to design and develop NLP, ML solutions. We used image pre-processing to improve OCR accuracy, further to integrate these solutions with RPA s'\n","Chunk 46: 'y, further to integrate these solutions with RPA system.\\nEducation PH.D | COMPUTER VISION | DEC 2016] IIIT, ALLAHABAD, INDIA\\nM.TECH | INFORMATION SECURITY | JUNE 2011 | KIIT UNIVERSITY, BHUBANESHWAR,'\n","Chunk 47: 'RITY | JUNE 2011 | KIIT UNIVERSITY, BHUBANESHWAR, INDIA M.SC | INFORMATION TECHNOLOGY | JUNE 2009 | KUMAUN UNIVERSITY, ALMORA, INDIA\\nB.SC | MATHEMATICS | JUNE 2007 | KUMAUN UNIVERSITY, ALMORA, INDIA\\nS'\n","Chunk 48: 'S | JUNE 2007 | KUMAUN UNIVERSITY, ALMORA, INDIA\\nSkills & Abilities\\n= Machine Learning = Machine learning operations (MLOps) = Computer vision = Certified scrum master\\n= Natural language processing ='\n","Chunk 49: 'fied scrum master\\n= Natural language processing = AWS, GCP, E2E cloud platforms\\n= Generative Al, LLM, Vision Language Model = Python, TensorFlow, Pytorch, MySQL\\nPage 2= Genetic Algorithms, Fuzzy Syste'\n","Chunk 50: 'rch, MySQL\\nPage 2= Genetic Algorithms, Fuzzy Systems = Human-Robot Interactions (Nao, Pepper)\\nCertifications\\n= Machine Learning Operations (MLOps) Coursera - Deeplearnining.ai Focus Areas - ML Workflo'\n","Chunk 51: 'rsera - Deeplearnining.ai Focus Areas - ML Workflows, TFX, Model Deployments and Tracking for Production May 3, 2022 = TensorFlow 2 for Deep Learning Coursera - Imperial College London Focus Areas â€” T'\n","Chunk 52: 'Coursera - Imperial College London Focus Areas â€” TensorFlow 2 APIs, Customized Model Training, Probabilistic Deep Learning Jul 9, 2023 = Machine Learning and Soft Computing Indian Statistical Institut'\n","Chunk 53: 'ing and Soft Computing Indian Statistical Institute, Kolkata, India Focus Areas - Artificial Neural Network, Computer Vision and Genetic Algorithms Dec 20, 2012\\nAchievements\\n= SOPHIA - A H2020 EU PROJ'\n","Chunk 54: 'c 20, 2012\\nAchievements\\n= SOPHIA - A H2020 EU PROJECT .\\nSecured three years post-doctoral position on deep learning perception for human-robot collaboration at LIRMM Montpellier.\\n= POST-DOCTORAL FUNDI'\n","Chunk 55: 'ration at LIRMM Montpellier.\\n= POST-DOCTORAL FUNDING FROM KEMPHE .\\nFOUNDATIONS, SWEDEN Received 2 years of research funding to pursue my postdoctoral work in Human-Robot Interaction at Umea University'\n","Chunk 56: 'work in Human-Robot Interaction at Umea University, Sweden.\\n= 1ST POSITION IN M.SC Secured first position in M.Sc for consecutive 2 years at university level.\\nProjects\\nGit, CI/CD, Dockers, Micro Servi'\n","Chunk 57: 'y level.\\nProjects\\nGit, CI/CD, Dockers, Micro Services (REST), JIRA 10T - ESP32, MQTT, Jetson Nano, Raspberry, NXP\\nBuilding Cloud Computing Solutions at Scale Coursera - Duke University\\nFocus Areas - C'\n","Chunk 58: 't Scale Coursera - Duke University\\nFocus Areas - Containers, Cloud APIs, DevOps, AWS an Azure\\nNov 6, 2023\\nScrum Master Certification Specialization Coursera â€” Learn Quest\\nFocus Areas - Agile Methodolo'\n","Chunk 59: 'ursera â€” Learn Quest\\nFocus Areas - Agile Methodology and Product Management\\nFeb 17, 2023\\nPrompt Engineering for ChatGPT\\nVanderbilt University\\nFocus Area - Few Shot Examples, Meta Language Creation Pat'\n","Chunk 60: 'ea - Few Shot Examples, Meta Language Creation Pattern, Different prompts patterns July 9, 2023\\nMARIE SKLODOWSKA CURIE ACTIONS (MSCA) 2020 Applied for the MSCA post-doctoral funding in association wit'\n","Chunk 61: 'the MSCA post-doctoral funding in association with Universitat Rovira i Virgili, Spain and received 85% marks.\\nMHRD, INDIA PH.D FELLOWSHIP\\nReceived 2 years of junior research and 2 years of senior re'\n","Chunk 62: \"years of junior research and 2 years of senior research funding from MHRD for the Ph.D program under robotics and AI lab, IIIT Allahabad.\\nFINETUNENING OF LLM FOR Q&A ON WEBSITE DATA\\nIn today's world,\"\n","Chunk 63: \"G OF LLM FOR Q&A ON WEBSITE DATA\\nIn today's world, simply having an FAQ section or static information on a website isn't enough. We can harness this data to build a conversational Al system that provi\"\n","Chunk 64: 'ata to build a conversational Al system that provides more effective and intuitive search and information retrieval. To tackle this, we fine-tuned the LLaMA model using data scraped from the website.'\n","Chunk 65: 'LLaMA model using data scraped from the website. Initially, we generated question- answer pairs using the website content and ChatGPT. Once we had sufficient data, we used it to fine-tune the model.'\n","Chunk 66: 'fficient data, we used it to fine-tune the model. The model was then deployed as a conversational Al system using OpenWebUI, enabling users to engage with the content in a more interactive manner.\\nTec'\n","Chunk 67: 'with the content in a more interactive manner.\\nTechniques Used: Llama, OpenWebUI, LoRA, Data Preparation, Tokenization and Encoding.\\nPage 3FINANCE LARGE LANGUAGE MODEL\\nWe fine-tuned BERT, an LLM model'\n","Chunk 68: 'GE LANGUAGE MODEL\\nWe fine-tuned BERT, an LLM model, to handle complex reasoning tasks found in financial reports. We utilized the FinQA dataset, which consists of earnings reports from S&P 500 compani'\n","Chunk 69: 'consists of earnings reports from S&P 500 companies spanning from 1999 to 2019. These reports, typically in PDF format, include multiple pages of financial data, often presented in tables and text. T'\n","Chunk 70: 'ancial data, often presented in tables and text. The model generates the mathematical equation necessary to conduct the calculation and further execute that equation on table selected values to get th'\n","Chunk 71: 'e that equation on table selected values to get the output value (execution value). The model achieved an execution accuracy of 65.05%.\\nTechniques Used: BERT, Language Embedding, Program Accuracy, Exe'\n","Chunk 72: 'd: BERT, Language Embedding, Program Accuracy, Execution Accuracy\\nSMART VISION - BRANE ENTERPRISES\\nWe created a smart eyewear prototype equipped with an integrated camera to aid visually impaired indi'\n","Chunk 73: 'an integrated camera to aid visually impaired individuals. Our in-house fabrication of the PCB board enables essential functionalities like Bluetooth connectivity, USB charging, and live-streaming to'\n","Chunk 74: 'connectivity, USB charging, and live-streaming to an Android device. Additionally, we developed a companion app providing features such as object detection, scene analysis, navigation assistance, and'\n","Chunk 75: 'ction, scene analysis, navigation assistance, and person counting. My role involved collaborating with the team to develop and integrate deep learning solutions into the app. Together, we successfully'\n","Chunk 76: 'solutions into the app. Together, we successfully implemented multiple models for object detection, image/dense captioning, currency recognition, OCR, and more.\\nTechniques Used: Faster-RCNN, MobileNe'\n","Chunk 77: ', and more.\\nTechniques Used: Faster-RCNN, MobileNetV2, CNN+LSTM, YOLO\\nINCREMENTAL FACE LEARNING - BRANE ENTERPRISES\\nWe developed an Android application for conducting face recognition using transfer l'\n","Chunk 78: 'n for conducting face recognition using transfer learning techniques. We utilized the FaceNet model as the backbone network and added a classification layer for face classification. One challenge with'\n","Chunk 79: 'layer for face classification. One challenge with neural networks is catastrophic forgetting, where training the entire network (excluding the backbone) on N+1 classes can lead to performance issues.'\n","Chunk 80: 'ne) on N+1 classes can lead to performance issues. To address this, we adopted an incremental learning approach that helps to train and deploy the model in 30 seconds reducing the onboarding time.\\nTec'\n","Chunk 81: 'el in 30 seconds reducing the onboarding time.\\nTechniques Used: Transfer Learning, FaceNet, Incremental Learning\\nACTION RECOGNITION - MONTPELLIER UNIVERSITY\\nGiven one second long measure of the humanâ€™'\n","Chunk 82: 'ERSITY\\nGiven one second long measure of the humanâ€™s motion, the system can determine human action. The originality lies in the use of joint angles, instead of cartesian coordinates. This design choice'\n","Chunk 83: 'stead of cartesian coordinates. This design choice makes the framework sensor agnostic and invariant to affine transformations and to anthropometric differences. On AnDy dataset, we outperform the sta'\n","Chunk 84: 'ifferences. On AnDy dataset, we outperform the state of the art classifier. Furthermore, we show that our system is effective with limited training data, that it is subject independent, and that it is'\n","Chunk 85: 'ta, that it is subject independent, and that it is compatible with robotic real time constraints. In terms of methodology, the system is an original synergy of two antithetical schools of thought: mod'\n","Chunk 86: 'ynergy of two antithetical schools of thought: model based and data-based algorithms. Indeed, it is the cascade of an inverse kinematics estimator compliant with the International Society of Biomechan'\n","Chunk 87: 'pliant with the International Society of Biomechanics recommendations, followed by a deep learning architecture based on Bidirectional Long Short Term Memory. Techniques Used: Bi-LSTM, Mocap, OpenPose'\n","Chunk 88: 'Memory. Techniques Used: Bi-LSTM, Mocap, OpenPose, CNN\\nVISUAL GROUNDING - UMEA UNIVERSITY\\nFor robots to engage with humans in real-world situations or with objects, they must develop a mental represe'\n","Chunk 89: 'r with objects, they must develop a mental representation (\"state of mind\") that a) reflects the robotsâ€™ perception and b) ideally aligns with human comprehension and ideas. Using table-top scenarios'\n","Chunk 90: 'omprehension and ideas. Using table-top scenarios as an example, we propose a framework for generating a robot\\'s \"state of mindâ€ by identifying the objects on the table along with their characteristic'\n","Chunk 91: \"jects on the table along with their characteristics (color, shape, texture) and spatial relationships to one another. The robot's view of the scene is depicted in a dynamic graph where object attribut\"\n","Chunk 92: 'depicted in a dynamic graph where object attributes are translated into fuzzy linguistic variables that correspond to human spatial concepts. This endeavor involves creating these graph representatio'\n","Chunk 93: 'deavor involves creating these graph representations through a combination of low-level neural network-based feature recognition and a high-level fuzzy inference system.\\nTechniques Used: Fuzzy Inferen'\n","Chunk 94: 'y inference system.\\nTechniques Used: Fuzzy Inference System, Mask-RCNN, CNN, Local Binary pattern (LBP), Multi-Layer Perceptron\\nPage 4Talks and Presentations\\n= Title: An empirical review of calibratio'\n","Chunk 95: 'tations\\n= Title: An empirical review of calibration techniques for the Pepper humanoid robotâ€™s RGB and depth camera. Venue: Intelligent systems and application, 5â€˜ Sep 2019, London, England. Occasion:'\n","Chunk 96: 'plication, 5â€˜ Sep 2019, London, England. Occasion: Presented conference paper in IntelliSys 2019. = Fusion of gesture and speech for increased accuracy in human robot interaction. Venue: 25th Internat'\n","Chunk 97: 'y in human robot interaction. Venue: 25th International conference on methods and models in automation and robotics, 24 Aug 2019, Miedzyzdroje, Poland. Occasion: Presented conference paper in MMAR 201'\n","Chunk 98: '. Occasion: Presented conference paper in MMAR 2019. = Conflict Detection and Resolution in Table Top Scenarios for Human-Robot Interaction. Venue: Computing science department, Umea University, 18 Ju'\n","Chunk 99: 'mputing science department, Umea University, 18 Jun, 2019, Umea, Sweden. Occasion: Poster presentation in 31st Swedish Al Society Workshop. = Deep learning and its applications. Venue: UFBI department'\n","Chunk 100: 'rning and its applications. Venue: UFBI department, Umea University, 15\" Jun 2018, Umea, Sweden. Occasion: Invited as a speaker at Umea center for Functional Brain Imaging (UFBI) day. = Sketch drawing'\n","Chunk 101: 'ctional Brain Imaging (UFBI) day. = Sketch drawing by NAO humanoid robot. Venue: TENCON a premier international technical conference of IEEE Region 10, 1st Nov, 2015, Macau, China. Occasion: Presented'\n","Chunk 102: ', 1st Nov, 2015, Macau, China. Occasion: Presented conference paper in TENCON 2015.\\nSelected Publications [ Journals and Conferences]\\nJOURNAL PUBLICATIONS\\n= Singh, A. K., Baranwal, N., Richter, K. F.,'\n","Chunk 103: 'IONS\\n= Singh, A. K., Baranwal, N., Richter, K. F., Hellstrom, T., & Bensch, S. (2020). Verbal explanations by collaborating robot teams. Paladyn, Journal of Behavioral Robotics, 12(1), 47-57.\\n= Singh,'\n","Chunk 104: 'nal of Behavioral Robotics, 12(1), 47-57.\\n= Singh, A. K., Baranwal, N., & Nandi, G. C. (2019). A rough set based reasoning approach for criminal identification. International Journal of Machine Learni'\n","Chunk 105: 'ification. International Journal of Machine Learning and Cybernetics, 10, 413-431.\\n= Baranwal, N., Nandi, G.C., & Singh, A. K. (2017). Real-Time Gesture-Based Communication Using Possibility Theory- B'\n","Chunk 106: 're-Based Communication Using Possibility Theory- Based Hidden Markov Model. Computational Intelligence, 33(4), 843-862.\\n= Baranwal, N., Singh, A. K., & Nandi, G. C. (2017). Development of a framework'\n","Chunk 107: '& Nandi, G. C. (2017). Development of a framework for human-robot interactions with Indian sign language using possibility theory. International Journal of Social Robotics, 9, 563-574.\\n= Singh, A. K.,'\n","Chunk 108: 'al of Social Robotics, 9, 563-574.\\n= Singh, A. K., Baranwal, N., & Nandi, G. C. (2017). Development of a self reliant humanoid robot for sketch drawing. Multimedia Tools and Applications, 76, 18847-18'\n","Chunk 109: 'g. Multimedia Tools and Applications, 76, 18847-18870.\\nCONFERENCE PUBLICATIONS\\n= Singh, A. K.,, Adjel, M., Bonnet, V., Passama, R., & Cherubini, A. (2022, November). A framework for recognizing indust'\n","Chunk 110: '022, November). A framework for recognizing industrial actions via joint angles. In 2022 IEEE-RAS 21st International Conference on Humanoid Robots (Humanoids) (pp. 210-216). IEEE.\\n= Kumar Singh, A., B'\n","Chunk 111: 'manoids) (pp. 210-216). IEEE.\\n= Kumar Singh, A., Baranwal, N., & Richter, K. F. (2020). A fuzzy inference system for a visually grounded robot state of mind. In ECAI 2020 (pp. 2402-2409). IOS Press.\\n='\n","Chunk 112: 'f mind. In ECAI 2020 (pp. 2402-2409). IOS Press.\\n= Singh, A. K., Baranwal, N., & Richter, K. F. (2020). An empirical review of calibration techniques for the pepper humanoid robotâ€™s RGB and depth came'\n","Chunk 113: 'for the pepper humanoid robotâ€™s RGB and depth camera. In Intelligent Systems and Applications: Proceedings of the 2019 Intelligent Systems Conference (IntelliSys) Volume 2 (pp. 1026-1038). Springer In'\n","Chunk 114: '(IntelliSys) Volume 2 (pp. 1026-1038). Springer International Publishing.\\n= Singh, A. K., Chakraborty, P., & Nandi, G. C. (2015, November). Sketch drawing by nao humanoid robot. In TENCON 2015-2015 IE'\n","Chunk 115: 'wing by nao humanoid robot. In TENCON 2015-2015 IEEE Region 10 Conference (pp. 1-6). IEEE.\\nPlease find the complete list of publication at my Google Scholar Profile.\\nOnline Presence\\nGithub: https://gi'\n","Chunk 116: 'cholar Profile.\\nOnline Presence\\nGithub: https://github.com/robaita LinkedIn: https://fr.linkedin.com/in/dr-avinash-kumar-singh-2a570a31\\nPage 5'\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"iJxdqc2P7Uah"},"execution_count":null,"outputs":[]}]}